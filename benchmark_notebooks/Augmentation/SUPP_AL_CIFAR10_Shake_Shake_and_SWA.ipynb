{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SUPP_AL_CIFAR10_Shake_Shake_and_SWA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vYmMCnfg1PN8"},"source":["# Preface\n","\n","The locations requiring configuration for your experiment are commented in capital text."]},{"cell_type":"markdown","metadata":{"id":"kgYWNPhf801A"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"-7DmzUo2vZZ_"},"source":["**Installations**"]},{"cell_type":"code","metadata":{"id":"wKMPt_L5bNeu"},"source":["!pip install apricot-select\n","!pip install sphinxcontrib-napoleon\n","!pip install sphinxcontrib-bibtex\n","\n","!git clone https://github.com/decile-team/distil.git\n","!git clone https://github.com/circulosmeos/gdown.pl.git\n","!git clone https://github.com/owruby/shake-shake_pytorch.git\n","\n","!mv distil asdf\n","!mv asdf/distil .\n","\n","!mv shake-shake_pytorch/models ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYsutkIJrGvK"},"source":["**Experiment-Specific Imports**"]},{"cell_type":"code","metadata":{"id":"lfQKdd0DrKsa"},"source":["from distil.utils.data_handler import DataHandler_CIFAR10, DataHandler_Points # IMPORT YOUR DATAHANDLER HERE\n","from models.shake_resnet import ShakeResNet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maz6VJxS787x"},"source":["**Imports, Training Class Definition, Experiment Procedure Definition**\n","\n","Nothing needs to be modified in this code block unless it specifically pertains to a change of experimental procedure."]},{"cell_type":"code","metadata":{"id":"V9-8qRo8KD3a"},"source":["import pandas as pd \n","import numpy as np\n","import copy\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Subset\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import torch\n","import torch.optim as optim\n","from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n","from torch.autograd import Variable\n","import sys\n","sys.path.append('../')\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import random\n","import os\n","import pickle\n","\n","from numpy.linalg import cond\n","from numpy.linalg import inv\n","from numpy.linalg import norm\n","from scipy import sparse as sp\n","from scipy.linalg import lstsq\n","from scipy.linalg import solve\n","from scipy.optimize import nnls\n","\n","from distil.active_learning_strategies.badge import BADGE\n","from distil.active_learning_strategies.glister import GLISTER\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.entropy_sampling import EntropySampling\n","from distil.active_learning_strategies.random_sampling import RandomSampling\n","from distil.active_learning_strategies.gradmatch_active import GradMatchActive\n","from distil.active_learning_strategies.craig_active import CRAIGActive\n","from distil.active_learning_strategies.fass import FASS\n","from distil.active_learning_strategies.adversarial_bim import AdversarialBIM\n","from distil.active_learning_strategies.adversarial_deepfool import AdversarialDeepFool\n","from distil.active_learning_strategies.core_set import CoreSet\n","from distil.active_learning_strategies.least_confidence import LeastConfidence\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout import BALDDropout\n","from distil.utils.dataset import get_dataset\n","\n","from google.colab import drive\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# -*- coding: utf-8 -*-\n","\n","from models.shakeshake import ShakeShake\n","from models.shakeshake import Shortcut\n","\n","class ShakeBlock(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch, stride=1):\n","        super(ShakeBlock, self).__init__()\n","        self.equal_io = in_ch == out_ch\n","        self.shortcut = self.equal_io and None or Shortcut(in_ch, out_ch, stride=stride)\n","\n","        self.branch1 = self._make_branch(in_ch, out_ch, stride)\n","        self.branch2 = self._make_branch(in_ch, out_ch, stride)\n","\n","    def forward(self, x):\n","        h1 = self.branch1(x)\n","        h2 = self.branch2(x)\n","        h = ShakeShake.apply(h1, h2, self.training)\n","        h0 = x if self.equal_io else self.shortcut(x)\n","        return h + h0\n","\n","    def _make_branch(self, in_ch, out_ch, stride=1):\n","        return nn.Sequential(\n","            nn.ReLU(inplace=False),\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=False),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, stride=1, bias=False),\n","            nn.BatchNorm2d(out_ch))\n","\n","\n","class ShakeResNet(nn.Module):\n","\n","    def __init__(self, depth, w_base, label):\n","        super(ShakeResNet, self).__init__()\n","        n_units = (depth - 2) / 6\n","\n","        in_chs = [16, w_base, w_base * 2, w_base * 4]\n","        self.in_chs = in_chs\n","\n","        self.c_in = nn.Conv2d(3, in_chs[0], 3, padding=1)\n","        self.layer1 = self._make_layer(n_units, in_chs[0], in_chs[1])\n","        self.layer2 = self._make_layer(n_units, in_chs[1], in_chs[2], 2)\n","        self.layer3 = self._make_layer(n_units, in_chs[2], in_chs[3], 2)\n","        self.fc_out = nn.Linear(in_chs[3], label)\n","\n","        # Initialize paramters\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, last=False):\n","        h = self.c_in(x)\n","        h = self.layer1(h)\n","        h = self.layer2(h)\n","        h = self.layer3(h)\n","        h = F.relu(h)\n","        h = F.avg_pool2d(h, 8)\n","        h = h.view(-1, self.in_chs[3])\n","        if last:\n","            output = self.fc_out(h)\n","            return output, h\n","        else:\n","            h = self.fc_out(h)\n","            return h\n","\n","    def get_embedding_dim(self):\n","        return self.fc_out.in_features\n","\n","    def _make_layer(self, n_units, in_ch, out_ch, stride=1):\n","        layers = []\n","        for i in range(int(n_units)):\n","            layers.append(ShakeBlock(in_ch, out_ch, stride=stride))\n","            in_ch, stride = out_ch, 1\n","        return nn.Sequential(*layers)\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","#custom training\n","class data_train:\n","\n","    def __init__(self, X, Y, net, handler, args):\n","\n","        self.X = X\n","        self.Y = Y\n","        self.net = net\n","        self.handler = handler\n","        self.args = args\n","        self.n_pool = len(Y)\n","        \n","        if 'islogs' not in args:\n","            self.args['islogs'] = False\n","\n","        if 'optimizer' not in args:\n","            self.args['optimizer'] = 'sgd'\n","        \n","        if 'isverbose' not in args:\n","            self.args['isverbose'] = False\n","        \n","        if 'isreset' not in args:\n","            self.args['isreset'] = True\n","\n","        if 'max_accuracy' not in args:\n","            self.args['max_accuracy'] = 0.95\n","\n","        if 'min_diff_acc' not in args: #Threshold to monitor for\n","            self.args['min_diff_acc'] = 0.001\n","\n","        if 'window_size' not in args:  #Window for monitoring accuracies\n","            self.args['window_size'] = 10\n","            \n","        if 'criterion' not in args:\n","            self.args['criterion'] = nn.CrossEntropyLoss()\n","            \n","        if 'device' not in args:\n","            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        else:\n","            self.device = args['device']\n","\n","    def update_index(self, idxs_lb):\n","        self.idxs_lb = idxs_lb\n","\n","    def update_data(self, X, Y):\n","    \tself.X = X\n","    \tself.Y = Y\n","\n","    def get_acc_on_set(self, X_test, Y_test):\n","        \n","        try:\n","            self.clf\n","        except:\n","            self.clf = self.net\n","\n","        if X_test is None:\n","            raise ValueError(\"Test data not present\")\n","        \n","        if Y_test is None:\n","            raise ValueError(\"Test labels not present\")\n","            \n","        if X_test.shape[0] != Y_test.shape[0]:\n","            raise ValueError(\"X_test has {self.X_test.shape[0]} values but {self.Y_test.shape[0]} labels\")\n","        \n","        if 'batch_size' in self.args:\n","            batch_size = self.args['batch_size']\n","        else:\n","            batch_size = 1 \n","        \n","        loader_te = DataLoader(self.handler(X_test, Y_test, False, use_test_transform=True), shuffle=False, pin_memory=True, batch_size=batch_size)\n","        self.clf.eval()\n","        accFinal = 0.\n","\n","        with torch.no_grad():        \n","            self.clf = self.clf.to(device=self.device)\n","            for batch_id, (x,y,idxs) in enumerate(loader_te):     \n","                x, y = x.to(device=self.device), y.to(device=self.device)\n","                out = self.clf(x)\n","                accFinal += torch.sum(1.0*(torch.max(out,1)[1] == y)).item() #.data.item()\n","\n","        return accFinal / len(loader_te.dataset.X)\n","\n","    def _train_weighted(self, epoch, loader_tr, optimizer, gradient_weights):\n","        self.clf.train()\n","        accFinal = 0.\n","        criterion = self.args['criterion']\n","        criterion.reduction = \"none\"\n","\n","        for batch_id, (x, y, idxs) in enumerate(loader_tr):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","            gradient_weights = gradient_weights.to(device=self.device)\n","\n","            optimizer.zero_grad()\n","            out = self.clf(x)\n","\n","            # Modify the loss function to apply weights before reducing to a mean\n","            loss = criterion(out, y.long())\n","\n","            # Perform a dot product with the loss vector and the weight vector, then divide by batch size.\n","            weighted_loss = torch.dot(loss, gradient_weights[idxs])\n","            weighted_loss = torch.div(weighted_loss, len(idxs))\n","\n","            accFinal += torch.sum(torch.eq(torch.max(out,1)[1],y)).item() #.data.item()\n","\n","            # Backward now does so on the weighted loss, not the regular mean loss\n","            weighted_loss.backward() \n","\n","            # clamp gradients, just in case\n","            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)\n","\n","            optimizer.step()\n","        return accFinal / len(loader_tr.dataset.X), weighted_loss\n","\n","    def _train(self, epoch, loader_tr, optimizer):\n","        self.clf.train()\n","        accFinal = 0.\n","        criterion = self.args['criterion']\n","\n","        for batch_id, (x, y, idxs) in enumerate(loader_tr):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","\n","            optimizer.zero_grad()\n","            out = self.clf(x)\n","            loss = criterion(out, y.long())\n","            accFinal += torch.sum((torch.max(out,1)[1] == y).float()).item()\n","            loss.backward()\n","\n","            # clamp gradients, just in case\n","            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)\n","\n","            optimizer.step()\n","        return accFinal / len(loader_tr.dataset.X), loss\n","\n","    def check_saturation(self, acc_monitor):\n","        \n","        saturate = True\n","\n","        for i in range(len(acc_monitor)):\n","            for j in range(i+1, len(acc_monitor)):\n","                if acc_monitor[j] - acc_monitor[i] >= self.args['min_diff_acc']:\n","                    saturate = False\n","                    break\n","\n","        return saturate\n","\n","    def train(self, gradient_weights=None):\n","\n","        print('Training..')\n","        def weight_reset(m):\n","            if hasattr(m, 'reset_parameters'):\n","                m.reset_parameters()\n","\n","        train_logs = []\n","        n_epoch = self.args['n_epoch']\n","        \n","        if self.args['isreset']:\n","            self.clf = self.net.apply(weight_reset).to(device=self.device)\n","        else:\n","            try:\n","                self.clf\n","            except:\n","                self.clf = self.net.apply(weight_reset).to(device=self.device)\n","\n","        if self.args['optimizer'] == 'sgd':\n","            optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)\n","            lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n","        \n","        elif self.args['optimizer'] == 'adam':\n","            optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)\n","\n","        # ADD stochastic weight averaging\n","        swa_sched = SWALR(optimizer, anneal_strategy=\"cos\", swa_lr=self.args['lr'])\n","        swa_model = AveragedModel(self.clf).to(device=self.device)\n","\n","        if 'batch_size' in self.args:\n","            batch_size = self.args['batch_size']\n","        else:\n","            batch_size = 1\n","\n","        # Set shuffle to true to encourage stochastic behavior for SGD\n","        loader_tr = DataLoader(self.handler(self.X, self.Y, False), batch_size=batch_size, shuffle=True, pin_memory=True)\n","        epoch = 1\n","        accCurrent = 0\n","        is_saturated = False\n","        acc_monitor = []\n","        start_swa = False\n","\n","        while (accCurrent < self.args['max_accuracy']) and (epoch < n_epoch) and (not is_saturated): \n","            \n","            if gradient_weights is None:\n","                accCurrent, lossCurrent = self._train(epoch, loader_tr, optimizer)\n","            else:\n","                accCurrent, lossCurrent = self._train_weighted(epoch, loader_tr, optimizer, gradient_weights)\n","            \n","            acc_monitor.append(accCurrent)\n","\n","            if accCurrent > 0.95:\n","                start_swa = True\n","\n","            if start_swa:\n","                swa_model.update_parameters(self.clf)\n","                swa_sched.step()\n","            elif self.args['optimizer'] == 'sgd':\n","                lr_sched.step()\n","\n","            epoch += 1\n","            if(self.args['isverbose']):\n","                if epoch % 50 == 0:\n","                    print(str(epoch) + ' training accuracy: ' + str(accCurrent), flush=True)\n","\n","            #Stop training if not converging\n","            if len(acc_monitor) >= self.args['window_size']:\n","\n","                is_saturated = self.check_saturation(acc_monitor)\n","                del acc_monitor[0]\n","\n","            log_string = 'Epoch:' + str(epoch) + '- training accuracy:'+str(accCurrent)+'- training loss:'+str(lossCurrent)\n","            train_logs.append(log_string)\n","            if (epoch % 50 == 0) and (accCurrent < 0.2): # resetif not converging\n","                self.clf = self.net.apply(weight_reset).to(device=self.device)\n","                \n","                if self.args['optimizer'] == 'sgd':\n","\n","                    optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)\n","                    lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n","\n","                else:\n","                    optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)\n","\n","        print('Epoch:', str(epoch), 'Training accuracy:', round(accCurrent, 3), flush=True)\n","\n","        # Update batch normalization and set averaged model to clf\n","        update_bn(loader_tr, swa_model, device=self.device)\n","        self.clf = swa_model.module\n","\n","        if self.args['islogs']:\n","            return self.clf, train_logs\n","        else:\n","            return self.clf\n","\n","class Checkpoint:\n","\n","    def __init__(self, acc_list=None, indices=None, state_dict=None, experiment_name=None, path=None):\n","\n","        # If a path is supplied, load a checkpoint from there.\n","        if path is not None:\n","\n","            if experiment_name is not None:\n","                self.load_checkpoint(path, experiment_name)\n","            else:\n","                raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","            return\n","\n","        if acc_list is None:\n","            raise ValueError(\"Checkpoint contains None value for acc_list\")\n","\n","        if indices is None:\n","            raise ValueError(\"Checkpoint contains None value for indices\")\n","\n","        if state_dict is None:\n","            raise ValueError(\"Checkpoint contains None value for state_dict\")\n","\n","        if experiment_name is None:\n","            raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","        self.acc_list = acc_list\n","        self.indices = indices\n","        self.state_dict = state_dict\n","        self.experiment_name = experiment_name\n","\n","    def __eq__(self, other):\n","\n","        # Check if the accuracy lists are equal\n","        acc_lists_equal = self.acc_list == other.acc_list\n","\n","        # Check if the indices are equal\n","        indices_equal = self.indices == other.indices\n","\n","        # Check if the experiment names are equal\n","        experiment_names_equal = self.experiment_name == other.experiment_name\n","\n","        return acc_lists_equal and indices_equal and experiment_names_equal\n","\n","    def save_checkpoint(self, path):\n","\n","        # Get current time to use in file timestamp\n","        timestamp = time.time_ns()\n","\n","        # Create the path supplied\n","        os.makedirs(path, exist_ok=True)\n","\n","        # Name saved files using timestamp to add recency information\n","        save_path = os.path.join(path, F\"c{timestamp}1\")\n","        copy_save_path = os.path.join(path, F\"c{timestamp}2\")\n","\n","        # Write this checkpoint to the first save location\n","        with open(save_path, 'wb') as save_file:\n","            pickle.dump(self, save_file)\n","\n","        # Write this checkpoint to the second save location\n","        with open(copy_save_path, 'wb') as copy_save_file:\n","            pickle.dump(self, copy_save_file)\n","\n","    def load_checkpoint(self, path, experiment_name):\n","\n","        # Obtain a list of all files present at the path\n","        timestamp_save_no = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","        # If there are no such files, set values to None and return\n","        if len(timestamp_save_no) == 0:\n","            self.acc_list = None\n","            self.indices = None\n","            self.state_dict = None\n","            return\n","\n","        # Sort the list of strings to get the most recent\n","        timestamp_save_no.sort(reverse=True)\n","\n","        # Read in two files at a time, checking if they are equal to one another. \n","        # If they are equal, then it means that the save operation finished correctly.\n","        # If they are not, then it means that the save operation failed (could not be \n","        # done atomically). Repeat this action until no possible pair can exist.\n","        while len(timestamp_save_no) > 1:\n","\n","            # Pop a most recent checkpoint copy\n","            first_file = timestamp_save_no.pop(0)\n","\n","            # Keep popping until two copies with equal timestamps are present\n","            while True:\n","                \n","                second_file = timestamp_save_no.pop(0)\n","                \n","                # Timestamps match if the removal of the \"1\" or \"2\" results in equal numbers\n","                if (second_file[:-1]) == (first_file[:-1]):\n","                    break\n","                else:\n","                    first_file = second_file\n","\n","                    # If there are no more checkpoints to examine, set to None and return\n","                    if len(timestamp_save_no) == 0:\n","                        self.acc_list = None\n","                        self.indices = None\n","                        self.state_dict = None\n","                        return\n","\n","            # Form the paths to the files\n","            load_path = os.path.join(path, first_file)\n","            copy_load_path = os.path.join(path, second_file)\n","\n","            # Load the two checkpoints\n","            with open(load_path, 'rb') as load_file:\n","                checkpoint = pickle.load(load_file)\n","\n","            with open(copy_load_path, 'rb') as copy_load_file:\n","                checkpoint_copy = pickle.load(copy_load_file)\n","\n","            # Do not check this experiment if it is not the one we need to restore\n","            if checkpoint.experiment_name != experiment_name:\n","                continue\n","\n","            # Check if they are equal\n","            if checkpoint == checkpoint_copy:\n","\n","                # This checkpoint will suffice. Populate this checkpoint's fields \n","                # with the selected checkpoint's fields.\n","                self.acc_list = checkpoint.acc_list\n","                self.indices = checkpoint.indices\n","                self.state_dict = checkpoint.state_dict\n","                return\n","\n","        # Instantiate None values in acc_list, indices, and model\n","        self.acc_list = None\n","        self.indices = None\n","        self.state_dict = None\n","\n","    def get_saved_values(self):\n","\n","        return (self.acc_list, self.indices, self.state_dict)\n","\n","def delete_checkpoints(checkpoint_directory, experiment_name):\n","\n","    # Iteratively go through each checkpoint, deleting those whose experiment name matches.\n","    timestamp_save_no = [f for f in os.listdir(checkpoint_directory) if os.path.isfile(os.path.join(checkpoint_directory, f))]\n","\n","    for file in timestamp_save_no:\n","\n","        delete_file = False\n","\n","        # Get file location\n","        file_path = os.path.join(checkpoint_directory, file)\n","\n","        if not os.path.exists(file_path):\n","            continue\n","\n","        # Unpickle the checkpoint and see if its experiment name matches\n","        with open(file_path, \"rb\") as load_file:\n","\n","            checkpoint_copy = pickle.load(load_file)\n","            if checkpoint_copy.experiment_name == experiment_name:\n","                delete_file = True\n","\n","        # Delete this file only if the experiment name matched\n","        if delete_file:\n","            os.remove(file_path)\n","\n","#Logs\n","def write_logs(logs, save_directory, rd, run):\n","  file_path = save_directory + 'run_'+str(run)+'.txt'\n","  with open(file_path, 'a') as f:\n","    f.write('---------------------\\n')\n","    f.write('Round '+str(rd)+'\\n')\n","    f.write('---------------------\\n')\n","    for key, val in logs.items():\n","      if key == 'Training':\n","        f.write(str(key)+ '\\n')\n","        for epoch in val:\n","          f.write(str(epoch)+'\\n')       \n","      else:\n","        f.write(str(key) + ' - '+ str(val) +'\\n')\n","\n","def train_one(X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, strategy, save_directory, run, checkpoint_directory, experiment_name):\n","\n","    # Define acc initially\n","    acc = np.zeros(n_rounds+1)\n","\n","    initial_unlabeled_size = X_unlabeled.shape[0]\n","\n","    initial_round = 1\n","\n","    # Define an index map\n","    index_map = np.array([x for x in range(initial_unlabeled_size)])\n","\n","    # Attempt to load a checkpoint. If one exists, then the experiment crashed.\n","    training_checkpoint = Checkpoint(experiment_name=experiment_name, path=checkpoint_directory)\n","    rec_acc, rec_indices, rec_state_dict = training_checkpoint.get_saved_values()\n","\n","    # Check if there are values to recover\n","    if rec_acc is not None:\n","\n","        # Restore the accuracy list\n","        for i in range(len(rec_acc)):\n","            acc[i] = rec_acc[i]\n","\n","        # Restore the indices list and shift those unlabeled points to the labeled set.\n","        index_map = np.delete(index_map, rec_indices)\n","\n","        # Record initial size of X_tr\n","        intial_seed_size = X_tr.shape[0]\n","\n","        X_tr = np.concatenate((X_tr, X_unlabeled[rec_indices]), axis=0)\n","        X_unlabeled = np.delete(X_unlabeled, rec_indices, axis = 0)\n","\n","        y_tr = np.concatenate((y_tr, y_unlabeled[rec_indices]), axis = 0)\n","        y_unlabeled = np.delete(y_unlabeled, rec_indices, axis = 0)\n","\n","        # Restore the model\n","        net.load_state_dict(rec_state_dict) \n","\n","        # Fix the initial round\n","        initial_round = (X_tr.shape[0] - initial_seed_size) // budget + 1\n","\n","        # Ensure loaded model is moved to GPU\n","        if torch.cuda.is_available():\n","            net = net.cuda()     \n","\n","        strategy.update_model(net)\n","        strategy.update_data(X_tr, y_tr, X_unlabeled) \n","\n","    else:\n","\n","        if torch.cuda.is_available():\n","            net = net.cuda()\n","\n","        acc[0] = dt.get_acc_on_set(X_test, y_test)\n","        print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n","\n","        logs = {}\n","        logs['Training Points'] = X_tr.shape[0]\n","        logs['Test Accuracy'] =  str(round(acc[0]*100, 2))\n","        write_logs(logs, save_directory, 0, run)\n","          \n","        #Updating the trained model in strategy class\n","        strategy.update_model(net)\n","\n","    ##User Controlled Loop\n","    for rd in range(initial_round, n_rounds+1):\n","        print('-------------------------------------------------')\n","        print('Round', rd) \n","        print('-------------------------------------------------')\n","\n","        sel_time = time.time()\n","        idx = strategy.select(budget)            \n","        sel_time = time.time() - sel_time\n","        print(\"Selection Time:\", sel_time)\n","\n","        #Saving state of model, since labeling new points might take time\n","        # strategy.save_state()\n","\n","        #Adding new points to training set\n","        X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n","        X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n","\n","        #Human In Loop, Assuming user adds new labels here\n","        y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n","        y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n","\n","        # Update the index map\n","        index_map = np.delete(index_map, idx, axis = 0)\n","\n","        print('Number of training points -',X_tr.shape[0])\n","\n","        #Reload state and start training\n","        # strategy.load_state()\n","        strategy.update_data(X_tr, y_tr, X_unlabeled)\n","        dt.update_data(X_tr, y_tr)\n","        t1 = time.time()\n","        clf, train_logs = dt.train(None)\n","        t2 = time.time()\n","        acc[rd] = dt.get_acc_on_set(X_test, y_test)\n","        logs = {}\n","        logs['Training Points'] = X_tr.shape[0]\n","        logs['Test Accuracy'] =  str(round(acc[rd]*100, 2))\n","        logs['Selection Time'] = str(sel_time)\n","        logs['Trainining Time'] = str(t2 - t1) \n","        logs['Training'] = train_logs\n","        write_logs(logs, save_directory, rd, run)\n","        strategy.update_model(clf)\n","        print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n","\n","        # Create a checkpoint\n","        used_indices = np.array([x for x in range(initial_unlabeled_size)])\n","        used_indices = np.delete(used_indices, index_map).tolist()\n","\n","        round_checkpoint = Checkpoint(acc.tolist(), used_indices, clf.state_dict(), experiment_name=experiment_name)\n","        round_checkpoint.save_checkpoint(checkpoint_directory)\n","\n","    print('Training Completed')\n","    return acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def BADGE_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BADGE(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def random_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = RandomSampling(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def entropy_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = EntropySampling(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def GLISTER_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","        \n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'lr': args['lr'], 'device':args['device']}\n","        strategy = GLISTER(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args,valid=False, typeOf='rand', lam=0.1)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def FASS_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = FASS(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def adversarial_bim_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialBIM(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def adversarial_deepfool_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialDeepFool(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def coreset_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = CoreSet(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def least_confidence_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = LeastConfidence(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def margin_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = MarginSampling(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def bald_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds + 1)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BALDDropout(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds + 1)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rFh9y0M3ZVH"},"source":["# CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"E-e_sDnGsC_N"},"source":["**Parameter Definitions**\n","\n","Parameters related to the specific experiment are placed here. You should examine each and modify them as needed."]},{"cell_type":"code","metadata":{"id":"0cHXLa_YsIQG"},"source":["data_set_name = 'CIFAR10'\n","download_path = '../downloaded_data/'\n","handler = DataHandler_CIFAR10 # PUT DATAHANDLER HERE\n","net = ShakeResNet(depth=14, w_base=32, label=10) # MODEL HERE\n","\n","# MODIFY AS NECESSARY\n","logs_directory = '/content/gdrive/MyDrive/colab_storage/logs/'\n","checkpoint_directory = '/content/gdrive/MyDrive/colab_storage/check/'\n","initial_model = data_set_name\n","model_directory = \"/content/gdrive/MyDrive/colab_storage/model/\"\n","\n","experiment_name = \"SHAKE-SHAKE AND SWA\"\n","\n","initial_seed_size = 1000 # INIT SEED SIZE HERE\n","training_size_cap = 25000 # TRAIN SIZE CAP HERE\n","\n","nclasses = 10 # NUM CLASSES HERE\n","budget = 3000 # BUDGET HERE\n","\n","# CHANGE ARGS AS NECESSARY\n","args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':float(0.99), 'num_classes':nclasses, 'islogs':True, 'isreset':True, 'isverbose':True, 'device':'cuda'} \n","\n","# Train on approximately the full dataset given the budget contraints\n","n_rounds = (training_size_cap - initial_seed_size) // budget\n","\n","# SET N EXP TO RUN (>1 for repeat)\n","n_exp = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0WfH3eq3nv_"},"source":["**Initial Loading and Training**\n","\n","You may choose to train a new initial model or to continue to load a specific model. If this notebook is being executed in Colab, you should consider whether or not you need the gdown line."]},{"cell_type":"code","metadata":{"id":"K1522SUk3nwF"},"source":["# Mount drive containing possible saved model and define file path.\n","colab_model_storage_mount = \"/content/gdrive\"\n","drive.mount(colab_model_storage_mount)\n","\n","# Retrieve the model from Apurva's link and save it to the drive\n","os.makedirs(logs_directory, exist_ok = True)\n","os.makedirs(checkpoint_directory, exist_ok = True)\n","os.makedirs(model_directory, exist_ok = True)\n","model_directory = F\"{model_directory}/{data_set_name}\"\n","#!/content/gdown.pl/gdown.pl \"clone link\" \"clone location\" # MAY NOT NEED THIS LINE IF NOT CLONING MODEL FROM COLAB\n","\n","X, y, X_test, y_test = get_dataset(data_set_name, download_path)\n","dim = np.shape(X)[1:]\n","\n","X_tr = X[:initial_seed_size]\n","y_tr = y[:initial_seed_size].numpy()\n","X_unlabeled = X[initial_seed_size:]\n","y_unlabeled = y[initial_seed_size:].numpy()\n","\n","X_test = X_test\n","y_test = y_test.numpy()\n","\n","# COMMENT OUT ONE OR THE OTHER IF YOU WANT TO TRAIN A NEW INITIAL MODEL\n","load_model = False\n","#load_model = True\n","\n","# Only train a new model if one does not exist.\n","if load_model:\n","    net.load_state_dict(torch.load(model_directory))\n","    dt = data_train(X_tr, y_tr, net, handler, args)\n","    clf = net\n","else:\n","    dt = data_train(X_tr, y_tr, net, handler, args)\n","    clf, _ = dt.train(None)\n","    torch.save(clf.state_dict(), model_directory)\n","\n","print(\"Training for\", n_rounds, \"rounds with budget\", budget, \"on unlabeled set size\", training_size_cap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9N-4eTMPrZZ"},"source":["**Random Sampling**"]},{"cell_type":"code","metadata":{"id":"i4eKSOaiPruO"},"source":["strat_logs = logs_directory+F'{data_set_name}/random_sampling/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_random = random_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, F\"{experiment_name}_random\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg1XH87hPsCe"},"source":["**Entropy (Uncertainty) Sampling**"]},{"cell_type":"code","metadata":{"id":"mRAKMe2RPsTp"},"source":["strat_logs = logs_directory+F'{data_set_name}/entropy_sampling/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_entropy = entropy_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, F\"{experiment_name}_entropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZSiRahu3nwK"},"source":["**BADGE**"]},{"cell_type":"code","metadata":{"id":"b5c8AckN3nwK"},"source":["strat_logs = logs_directory+F'{data_set_name}/badge/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_badge = BADGE_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, F\"{experiment_name}_badge\")"],"execution_count":null,"outputs":[]}]}