{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Redundancy_experiments.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kgYWNPhf801A"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"-7DmzUo2vZZ_"},"source":["**Installations**"]},{"cell_type":"code","metadata":{"id":"wKMPt_L5bNeu"},"source":["!pip install apricot-select\n","\n","!git clone https://github.com/decile-team/distil.git\n","!git clone https://github.com/circulosmeos/gdown.pl.git\n","\n","!mv distil asdf\n","!mv asdf/distil ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maz6VJxS787x"},"source":["**Imports, Training Class Definition, Experiment Procedure Definition**"]},{"cell_type":"code","metadata":{"id":"V9-8qRo8KD3a"},"source":["import pandas as pd \n","import numpy as np\n","import copy\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Subset\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import torch\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import sys\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import random\n","import os\n","import pickle\n","\n","from numpy.linalg import cond\n","from numpy.linalg import inv\n","from numpy.linalg import norm\n","from scipy import sparse as sp\n","from scipy.linalg import lstsq\n","from scipy.linalg import solve\n","from scipy.optimize import nnls\n","\n","from distil.active_learning_strategies.badge import BADGE\n","from distil.active_learning_strategies.entropy_sampling import EntropySampling\n","from distil.active_learning_strategies.random_sampling import RandomSampling\n","\n","from distil.utils.models.resnet import ResNet18\n","from distil.utils.data_handler import DataHandler_CIFAR10, DataHandler_SVHN\n","from distil.utils.dataset import get_dataset, make_data_redundant, make_aug_data_redundant\n","from distil.utils.train_helper import data_train\n","\n","from google.colab import drive\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","class Checkpoint:\n","\n","    def __init__(self, acc_list=None, indices=None, state_dict=None, experiment_name=None, path=None):\n","\n","        # If a path is supplied, load a checkpoint from there.\n","        if path is not None:\n","\n","            if experiment_name is not None:\n","                self.load_checkpoint(path, experiment_name)\n","            else:\n","                raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","            return\n","\n","        if acc_list is None:\n","            raise ValueError(\"Checkpoint contains None value for acc_list\")\n","\n","        if indices is None:\n","            raise ValueError(\"Checkpoint contains None value for indices\")\n","\n","        if state_dict is None:\n","            raise ValueError(\"Checkpoint contains None value for state_dict\")\n","\n","        if experiment_name is None:\n","            raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","        self.acc_list = acc_list\n","        self.indices = indices\n","        self.state_dict = state_dict\n","        self.experiment_name = experiment_name\n","\n","    def __eq__(self, other):\n","\n","        # Check if the accuracy lists are equal\n","        acc_lists_equal = self.acc_list == other.acc_list\n","\n","        # Check if the indices are equal\n","        indices_equal = self.indices == other.indices\n","\n","        # Check if the experiment names are equal\n","        experiment_names_equal = self.experiment_name == other.experiment_name\n","\n","        return acc_lists_equal and indices_equal and experiment_names_equal\n","\n","    def save_checkpoint(self, path):\n","\n","        # Get current time to use in file timestamp\n","        timestamp = time.time_ns()\n","\n","        # Create the path supplied\n","        os.makedirs(path, exist_ok=True)\n","\n","        # Name saved files using timestamp to add recency information\n","        save_path = os.path.join(path, F\"c{timestamp}1\")\n","        copy_save_path = os.path.join(path, F\"c{timestamp}2\")\n","\n","        # Write this checkpoint to the first save location\n","        with open(save_path, 'wb') as save_file:\n","            pickle.dump(self, save_file)\n","\n","        # Write this checkpoint to the second save location\n","        with open(copy_save_path, 'wb') as copy_save_file:\n","            pickle.dump(self, copy_save_file)\n","\n","    def load_checkpoint(self, path, experiment_name):\n","\n","        # Obtain a list of all files present at the path\n","        timestamp_save_no = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","        # If there are no such files, set values to None and return\n","        if len(timestamp_save_no) == 0:\n","            self.acc_list = None\n","            self.indices = None\n","            self.state_dict = None\n","            return\n","\n","        # Sort the list of strings to get the most recent\n","        timestamp_save_no.sort(reverse=True)\n","\n","        # Read in two files at a time, checking if they are equal to one another. \n","        # If they are equal, then it means that the save operation finished correctly.\n","        # If they are not, then it means that the save operation failed (could not be \n","        # done atomically). Repeat this action until no possible pair can exist.\n","        while len(timestamp_save_no) > 1:\n","\n","            # Pop a most recent checkpoint copy\n","            first_file = timestamp_save_no.pop(0)\n","\n","            # Keep popping until two copies with equal timestamps are present\n","            while True:\n","                \n","                second_file = timestamp_save_no.pop(0)\n","                \n","                # Timestamps match if the removal of the \"1\" or \"2\" results in equal numbers\n","                if (second_file[:-1]) == (first_file[:-1]):\n","                    break\n","                else:\n","                    first_file = second_file\n","\n","                    # If there are no more checkpoints to examine, set to None and return\n","                    if len(timestamp_save_no) == 0:\n","                        self.acc_list = None\n","                        self.indices = None\n","                        self.state_dict = None\n","                        return\n","\n","            # Form the paths to the files\n","            load_path = os.path.join(path, first_file)\n","            copy_load_path = os.path.join(path, second_file)\n","\n","            # Load the two checkpoints\n","            with open(load_path, 'rb') as load_file:\n","                checkpoint = pickle.load(load_file)\n","\n","            with open(copy_load_path, 'rb') as copy_load_file:\n","                checkpoint_copy = pickle.load(copy_load_file)\n","\n","            # Do not check this experiment if it is not the one we need to restore\n","            if checkpoint.experiment_name != experiment_name:\n","                continue\n","\n","            # Check if they are equal\n","            if checkpoint == checkpoint_copy:\n","\n","                # This checkpoint will suffice. Populate this checkpoint's fields \n","                # with the selected checkpoint's fields.\n","                self.acc_list = checkpoint.acc_list\n","                self.indices = checkpoint.indices\n","                self.state_dict = checkpoint.state_dict\n","                return\n","\n","        # Instantiate None values in acc_list, indices, and model\n","        self.acc_list = None\n","        self.indices = None\n","        self.state_dict = None\n","\n","    def get_saved_values(self):\n","\n","        return (self.acc_list, self.indices, self.state_dict)\n","\n","def delete_checkpoints(checkpoint_directory, experiment_name):\n","\n","    # Iteratively go through each checkpoint, deleting those whose experiment name matches.\n","    timestamp_save_no = [f for f in os.listdir(checkpoint_directory) if os.path.isfile(os.path.join(checkpoint_directory, f))]\n","\n","    for file in timestamp_save_no:\n","\n","        delete_file = False\n","\n","        # Get file location\n","        file_path = os.path.join(checkpoint_directory, file)\n","\n","        # Unpickle the checkpoint and see if its experiment name matches\n","        with open(file_path, \"rb\") as load_file:\n","\n","            checkpoint_copy = pickle.load(load_file)\n","            if checkpoint_copy.experiment_name == experiment_name:\n","                delete_file = True\n","\n","        # Delete this file only if the experiment name matched\n","        if delete_file:\n","            os.remove(file_path)\n","\n","#Logs\n","def write_logs(logs, save_directory, rd, run):\n","  file_path = save_directory + 'run_'+str(run)+'.txt'\n","  with open(file_path, 'a') as f:\n","    f.write('---------------------\\n')\n","    f.write('Round '+str(rd)+'\\n')\n","    f.write('---------------------\\n')\n","    for key, val in logs.items():\n","      if key == 'Training':\n","        f.write(str(key)+ '\\n')\n","        for epoch in val:\n","          f.write(str(epoch)+'\\n')       \n","      else:\n","        f.write(str(key) + ' - '+ str(val) +'\\n')\n","\n","def train_one(X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, strategy, save_directory, run, checkpoint_directory, experiment_name):\n","\n","    # Define acc initially\n","    acc = np.zeros(n_rounds)\n","\n","    initial_unlabeled_size = X_unlabeled.shape[0]\n","\n","    initial_round = 1\n","\n","    # Define an index map\n","    index_map = np.array([x for x in range(initial_unlabeled_size)])\n","\n","    # Attempt to load a checkpoint. If one exists, then the experiment crashed.\n","    training_checkpoint = Checkpoint(experiment_name=experiment_name, path=checkpoint_directory)\n","    rec_acc, rec_indices, rec_state_dict = training_checkpoint.get_saved_values()\n","\n","    # Check if there are values to recover\n","    if rec_acc is not None:\n","\n","        # Restore the accuracy list\n","        for i in range(len(rec_acc)):\n","            acc[i] = rec_acc[i]\n","\n","        # Restore the indices list and shift those unlabeled points to the labeled set.\n","        index_map = np.delete(index_map, rec_indices)\n","\n","        # Record initial size of X_tr\n","        intial_seed_size = X_tr.shape[0]\n","\n","        X_tr = np.concatenate((X_tr, X_unlabeled[rec_indices]), axis=0)\n","        X_unlabeled = np.delete(X_unlabeled, rec_indices, axis = 0)\n","\n","        y_tr = np.concatenate((y_tr, y_unlabeled[rec_indices]), axis = 0)\n","        y_unlabeled = np.delete(y_unlabeled, rec_indices, axis = 0)\n","\n","        # Restore the model\n","        net.load_state_dict(rec_state_dict) \n","\n","        # Fix the initial round\n","        initial_round = (X_tr.shape[0] - initial_seed_size) // budget + 1\n","\n","        # Ensure loaded model is moved to GPU\n","        if torch.cuda.is_available():\n","            net = net.cuda()     \n","\n","        strategy.update_model(net)\n","        strategy.update_data(X_tr, y_tr, X_unlabeled) \n","\n","    else:\n","\n","        if torch.cuda.is_available():\n","            net = net.cuda()\n","\n","        acc[0] = dt.get_acc_on_set(X_test, y_test)\n","        print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n","\n","        logs = {}\n","        logs['Training Points'] = X_tr.shape[0]\n","        logs['Test Accuracy'] =  str(round(acc[0]*100, 2))\n","        write_logs(logs, save_directory, 0, run)\n","          \n","        #Updating the trained model in strategy class\n","        strategy.update_model(net)\n","\n","    ##User Controlled Loop\n","    for rd in range(initial_round, n_rounds):\n","        print('-------------------------------------------------')\n","        print('Round', rd) \n","        print('-------------------------------------------------')\n","\n","        sel_time = time.time()\n","        idx = strategy.select(budget)            \n","        sel_time = time.time() - sel_time\n","        print(\"Selection Time:\", sel_time)\n","\n","        #Saving state of model, since labeling new points might take time\n","        # strategy.save_state()\n","\n","        #Adding new points to training set\n","        X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n","        X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n","\n","        #Human In Loop, Assuming user adds new labels here\n","        y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n","        y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n","\n","        # Update the index map\n","        index_map = np.delete(index_map, idx, axis = 0)\n","\n","        print('Number of training points -',X_tr.shape[0])\n","\n","        #Reload state and start training\n","        # strategy.load_state()\n","        strategy.update_data(X_tr, y_tr, X_unlabeled)\n","        dt.update_data(X_tr, y_tr)\n","        t1 = time.time()\n","        clf, train_logs = dt.train(None)\n","        t2 = time.time()\n","        acc[rd] = dt.get_acc_on_set(X_test, y_test)\n","        logs = {}\n","        logs['Training Points'] = X_tr.shape[0]\n","        logs['Test Accuracy'] =  str(round(acc[rd]*100, 2))\n","        logs['Selection Time'] = str(sel_time)\n","        logs['Trainining Time'] = str(t2 - t1) \n","        logs['Training'] = train_logs\n","        write_logs(logs, save_directory, rd, run)\n","        strategy.update_model(clf)\n","        print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n","\n","        # Create a checkpoint\n","        used_indices = np.array([x for x in range(initial_unlabeled_size)])\n","        used_indices = np.delete(used_indices, index_map).tolist()\n","\n","        round_checkpoint = Checkpoint(acc.tolist(), used_indices, clf.state_dict(), experiment_name=experiment_name)\n","        round_checkpoint.save_checkpoint(checkpoint_directory)\n","\n","    print('Training Completed')\n","    return acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def BADGE_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size']}\n","        strategy = BADGE(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def random_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size']}\n","        strategy = RandomSampling(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n","\n","# Define a function to perform experiments in bulk and return the mean accuracies\n","def entropy_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, net, n_rounds, budget, args, nclasses, save_directory, checkpoint_directory, experiment_name):\n","\n","    test_acc_list = list()\n","    fig = plt.figure(figsize=(8,6), dpi=160)\n","    x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds)]\n","\n","    for i in range(n_exp):\n","        # Copy data and model to ensure that experiments do not override base versions\n","        X_tr_copy = copy.deepcopy(X_tr)\n","        y_tr_copy = copy.deepcopy(y_tr)\n","        X_unlabeled_copy = copy.deepcopy(X_unlabeled)\n","        y_unlabeled_copy = copy.deepcopy(y_unlabeled)\n","        X_test_copy = copy.deepcopy(X_test)\n","        y_test_copy = copy.deepcopy(y_test)\n","        dt_copy = copy.deepcopy(dt)\n","        clf_copy = copy.deepcopy(net)\n","\n","        #Initializing Strategy Class\n","        strategy_args = {'batch_size' : args['batch_size']}\n","        strategy = EntropySampling(X_tr, y_tr, X_unlabeled, net, handler, nclasses, strategy_args)\n","\n","        test_acc = train_one(X_tr_copy, y_tr_copy, X_test_copy, y_test_copy, X_unlabeled_copy, y_unlabeled_copy, dt_copy, clf_copy, n_rounds, budget, args, nclasses, strategy, save_directory, i, checkpoint_directory, experiment_name)\n","        test_acc_list.append(test_acc)\n","        plt.plot(x_axis, test_acc, label=str(i))\n","        print(\"EXPERIMENT\", i, test_acc)\n","\n","        # Experiment complete; delete all checkpoints related to this experiment\n","        delete_checkpoints(checkpoint_directory, experiment_name)\n","\n","    mean_test_acc = np.zeros(n_rounds)\n","\n","    for test_acc in test_acc_list:\n","        mean_test_acc = mean_test_acc + test_acc\n","\n","    mean_test_acc = mean_test_acc / n_exp\n","    plt.plot(x_axis, mean_test_acc, label=\"Mean\")\n","\n","    plt.xlabel(\"Labeled Set Size\")\n","    plt.ylabel(\"Test Acc\")\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"MEAN TEST ACC\", mean_test_acc)\n","\n","    return mean_test_acc\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sheUsNGPSeZN"},"source":["# Mount drive containing possible saved model and define file path (if needed)\n","colab_model_storage_mount = \"/content/gdrive\"\n","drive.mount(colab_model_storage_mount)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0WfH3eq3nv_"},"source":["**Initial Training and Parameter Definitions**"]},{"cell_type":"code","metadata":{"id":"K1522SUk3nwF"},"source":["data_set_name = 'CIFAR10' #'SVHN' #\n","download_path = '../downloaded_data/'\n","handler = DataHandler_CIFAR10 #DataHandler_SVHN\n","net = ResNet18()\n","\n","X, y, X_test, y_test = get_dataset(data_set_name, download_path)\n","\n","#Mount your Drive if needed\n","\n","# MODIFY AS NECESSARY\n","###################################\n","initial_seed_size = 500\n","training_size_cap = 5000\n","\n","\n","amtRed=20\n","if_aug = True   ##Set it to True if Augmented Duplicate are needed\n","\n","if if_aug:\n","  \n","  aug_path = F\"{download_path}{data_set_name}_aug_{amtRed}\" ##Location where Augmented Duplicate would be stored\n","\n","  os.makedirs(aug_path, exist_ok = True)\n","\n","  if os.path.exists(aug_path+'/train.npy'):\n","      X = np.load(aug_path+'/train.npy')\n","      y = np.load(aug_path+'/train_cat.npy')\n","  else:\n","      X=make_aug_data_redundant(X,y,initial_seed_size,amtRed=amtRed)\n","      np.save(aug_path+'/train.npy',X)\n","      np.save(aug_path+'/train_cat.npy',y)\n","\n","else:\n","  \n","  X = make_data_redundant(X,y,initial_seed_size,amtRed=amtRed)\n","\n","\n","X_tr = X[:initial_seed_size]\n","if not isinstance(y,np.ndarray):\n","    y_tr = y[:initial_seed_size].numpy()\n","else:\n","    y_tr = y[:initial_seed_size]\n","X_unlabeled = X[initial_seed_size:]\n","if not isinstance(y,np.ndarray):\n","    y_unlabeled = y[initial_seed_size:].numpy()\n","else:\n","    y_unlabeled = y[initial_seed_size:]\n","\n","X_test = X_test\n","y_test = y_test.numpy()\n","\n","nclasses = 10\n","budget = 500 \n","\n","logs_directory = '/content/logs/' ##Location where logs would be stored\n","checkpoint_directory = '/content/check/'    ##Location where checkpoints would be stored\n","initial_model = data_set_name\n","model_directory = \"/content/model/\"         ##Location where model would be stored\n","os.makedirs(logs_directory, exist_ok = True)\n","os.makedirs(checkpoint_directory, exist_ok = True)\n","os.makedirs(model_directory, exist_ok = True)\n","\n","model_directory = F\"/content/model/Intial_model\" ##Add path to your intial model\n","\n","#Initial Training\n","args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':float(0.99), 'num_classes':nclasses, 'islogs':True, 'isreset':True, 'isverbose':True} \n","\n","load_model = False   ## Determines whether `model_directory` is to be used to load or store \n","###################################\n","\n","\n","# Only train a new model if one does not exist.\n","if load_model:\n","    net.load_state_dict(torch.load(model_directory))\n","    dt = data_train(X_tr, y_tr, net, handler, args)\n","    clf = net\n","else:\n","    dt = data_train(X_tr, y_tr, net, handler, args)\n","    clf,_ = dt.train(None)\n","    torch.save(clf.state_dict(), model_directory)\n","\n","# Train on approximately the full dataset given the budget contraints\n","n_rounds = math.floor(training_size_cap / budget) + 1\n","n_exp = 3\n","\n","print(\"Training for\", n_rounds, \"rounds with budget\", budget, \"on unlabeled set size\", training_size_cap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9N-4eTMPrZZ"},"source":["**Random Sampling**"]},{"cell_type":"code","metadata":{"id":"i4eKSOaiPruO"},"source":["#delete_checkpoints(checkpoint_directory, \"c10_random\")\n","strat_logs = logs_directory+'random_sampling/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_random = random_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, \"c10_random\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg1XH87hPsCe"},"source":["**Entropy (Uncertainty) Sampling**"]},{"cell_type":"code","metadata":{"id":"mRAKMe2RPsTp"},"source":["#delete_checkpoints(checkpoint_directory, \"c10_entropy\")\n","strat_logs = logs_directory+'entropy_sampling/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_entropy = entropy_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, \"c10_entropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZSiRahu3nwK"},"source":["**BADGE**"]},{"cell_type":"code","metadata":{"id":"b5c8AckN3nwK"},"source":["strat_logs = logs_directory+'badge/'\n","os.makedirs(strat_logs, exist_ok = True)\n","mean_test_acc_badge = BADGE_experiment_batch(n_exp, X_tr, y_tr, X_test, y_test, X_unlabeled, y_unlabeled, dt, clf, n_rounds, budget, args, nclasses, strat_logs, checkpoint_directory, \"c10_badge\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDOMvj2f3nwK"},"source":["**Plot**\n","\n","If the runtime disconnects or if you have multiple instances of this notebook running, copy the cell output results (MEAN TEST ACC) and assign the accuracies to the appropriate variable below."]},{"cell_type":"code","metadata":{"id":"Ede-2d-x3nwL"},"source":["x_axis = [np.shape(X_tr)[0] + budget * x for x in range(0, n_rounds)]\n","\n","fig = plt.figure(figsize=(8,6), dpi=160)\n","\n","plt.plot(x_axis, mean_test_acc_random, color=(1,1,0), label=\"Random\")\n","plt.plot(x_axis, mean_test_acc_entropy, color=(1,0,1), label=\"Entropy\")\n","plt.plot(x_axis, mean_test_acc_badge, color=(0,0,0), label=\"BADGE\")\n","plt.xlabel(\"Labeled Set Size\")\n","plt.ylabel(\"Test Acc\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]}]}