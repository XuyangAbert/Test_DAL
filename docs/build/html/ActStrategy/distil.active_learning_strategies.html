

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Active Learning Strategies &mdash; DISTIL v0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Utilities" href="distil.utils.html" />
    <link rel="prev" title="DISTIL" href="modules.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> DISTIL
          

          
            
            <img src="../_static/distil_logo.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">DISTIL</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Active Learning Strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.adversarial_bim">Adversarial BIM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.adversarial_deepfool">Adversarial DeepFool</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.badge">BADGE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.batch_bald">BatchBALD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout">Bayesian Active Learning Disagreement Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.core_set">Core-Set Approch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.entropy_sampling">Entropy Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.entropy_sampling_dropout">Entropy Sampling with Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.fass">FASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.glister">GLISTER</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.gradmatch_active">GRADMATCH</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.kmeans_sampling">KMeans Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.least_confidence_sampling">Least Confidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.least_confidence_sampling_dropout">Least Confidence with Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.margin_sampling">Margin Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.margin_sampling_dropout">Margin Sampling with Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.partition_strategy">Partitioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.random_sampling">Random Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.scg">Submodular Conditional Gain (SCG)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.scmi">Submodular Conditional Mutual Information (SCMI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.smi">Submodular Mutual Information (SMI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-distil.active_learning_strategies.submod_sampling">Submodular Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">REFERENCES</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distil.utils.html">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="distil.utils.models.html">Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration Files for Training</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DISTIL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">DISTIL</a> &raquo;</li>
        
      <li>Active Learning Strategies</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/ActStrategy/distil.active_learning_strategies.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="active-learning-strategies">
<h1>Active Learning Strategies<a class="headerlink" href="#active-learning-strategies" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-distil.active_learning_strategies.adversarial_bim">
<span id="adversarial-bim"></span><h2>Adversarial BIM<a class="headerlink" href="#module-distil.active_learning_strategies.adversarial_bim" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.adversarial_bim.AdversarialBIM">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.adversarial_bim.</code><code class="sig-name descname">AdversarialBIM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/adversarial_bim.html#AdversarialBIM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.adversarial_bim.AdversarialBIM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implements Adversial Bim Strategy which is motivated by the fact that often the distance
computation from decision boundary is difficult and intractable for margin based methods. This
technique avoids estimating distance by using BIM(Basic Iterative Method)
<a class="footnote-reference brackets" href="#tramer2017ensemble" id="id1">1</a> to estimate how much adversarial perturbation is required to
cross the boundary. Smaller the required the perturbation, closer the point is to the boundary.</p>
<p><strong>Basic Iterative Method (BIM)</strong>: Given a base input, the approach is to perturb each
feature in the direction of the gradient by magnitude <span class="math notranslate nohighlight">\(\epsilon\)</span>, where is a
parameter that determines perturbation size. For a model with loss
<span class="math notranslate nohighlight">\(\nabla J(\theta, x, y)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> represents the model parameters,
x is the model input, and y is the label of x, the adversarial sample is generated
iteratively as,</p>
<div class="math notranslate nohighlight">
\[\begin{eqnarray}
    x^*_0 &amp; = &amp;x,
    x^*_i &amp; = &amp; clip_{x,e} (x^*_{i-1} + sign(\nabla_{x^*_{i-1}} J(\theta, x^*_{i-1} , y)))
\end{eqnarray}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: Batch size to be used inside strategy class (int, optional)</p></li>
<li><p><strong>device</strong>: The device that this strategy class should use for computation (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss that should be used for relevant computations (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>eps</strong>: Epsilon value for gradients (float, optional)</p></li>
<li><p><strong>verbose</strong>: Whether to print more output (bool, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.adversarial_bim.AdversarialBIM.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/adversarial_bim.html#AdversarialBIM.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.adversarial_bim.AdversarialBIM.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.adversarial_deepfool">
<span id="adversarial-deepfool"></span><h2>Adversarial DeepFool<a class="headerlink" href="#module-distil.active_learning_strategies.adversarial_deepfool" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.adversarial_deepfool.AdversarialDeepFool">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.adversarial_deepfool.</code><code class="sig-name descname">AdversarialDeepFool</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/adversarial_deepfool.html#AdversarialDeepFool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.adversarial_deepfool.AdversarialDeepFool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implements Adversial Deep Fool Strategy <a class="footnote-reference brackets" href="#ducoffe2018adversarial" id="id2">2</a>, a Deep-Fool based
Active Learning strategy that selects unlabeled samples with the smallest adversarial
perturbation. This technique is motivated by the fact that often the distance computation
from decision boundary is difficult and intractable for margin-based methods. This
technique avoids estimating distance by using Deep-Fool <a class="footnote-reference brackets" href="#moosavi-dezfooli-2016-cvpr" id="id3">3</a>
like techniques to estimate how much adversarial perturbation is required to cross the boundary.
The smaller the required perturbation, the closer the point is to the boundary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>max_iter</strong>: Maximum Number of Iterations (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.adversarial_deepfool.AdversarialDeepFool.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/adversarial_deepfool.html#AdversarialDeepFool.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.adversarial_deepfool.AdversarialDeepFool.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.badge">
<span id="badge"></span><h2>BADGE<a class="headerlink" href="#module-distil.active_learning_strategies.badge" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.badge.BADGE">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.badge.</code><code class="sig-name descname">BADGE</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/badge.html#BADGE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.badge.BADGE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This method is based on the paper Deep Batch Active Learning by Diverse, Uncertain Gradient
Lower Bounds <a class="footnote-reference brackets" href="#dblp-badge" id="id4">4</a>. According to the paper, this strategy, Batch Active
learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate
and high magnitude when represented in a hallucinated gradient space, a strategy designed to
incorporate both predictive uncertainty and sample diversity into every selected batch.
Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned
hyperparameters. Here at each round of selection, loss gradients are computed using the
hypothesised labels. Then to select the points to be labeled are selected by applying
k-means++ on these loss gradients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.badge.BADGE.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/badge.html#BADGE.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.badge.BADGE.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.batch_bald">
<span id="batchbald"></span><h2>BatchBALD<a class="headerlink" href="#module-distil.active_learning_strategies.batch_bald" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.batch_bald.BatchBALDDropout">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.batch_bald.</code><code class="sig-name descname">BatchBALDDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/batch_bald.html#BatchBALDDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.batch_bald.BatchBALDDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implementation of BatchBALD Strategy <a class="footnote-reference brackets" href="#kirsch2019batchbald" id="id5">5</a>, which refines
the original BALD acquisition to the batch setting using a new acquisition function.
This class extends <code class="xref py py-class docutils literal notranslate"><span class="pre">active_learning_strategies.strategy.Strategy</span></code>
to include a MC sampling technique based on the sampling techniques used in their paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>n_drop</strong>: Number of dropout runs to use to generate MC samples (int, optional)</p></li>
<li><p><strong>n_samples</strong>: Number of samples to use in computing joint entropy (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.batch_bald.BatchBALDDropout.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/batch_bald.html#BatchBALDDropout.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.batch_bald.BatchBALDDropout.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout">
<span id="bayesian-active-learning-disagreement-dropout"></span><h2>Bayesian Active Learning Disagreement Dropout<a class="headerlink" href="#module-distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout.BALDDropout">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout.</code><code class="sig-name descname">BALDDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/bayesian_active_learning_disagreement_dropout.html#BALDDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout.BALDDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements Bayesian Active Learning by Disagreement (BALD) Strategy <a class="footnote-reference brackets" href="#houlsby2011bayesian" id="id6">6</a>,
which assumes a Basiyan setting and selects points which maximise the mutual information
between the predicted labels and model parameters. This implementation is an adaptation for a
non-bayesian setting, with the assumption that there is a dropout layer in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>n_drop</strong>: Number of dropout runs to use to generate MC samples (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout.BALDDropout.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout.BALDDropout.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.core_set">
<span id="core-set-approch"></span><h2>Core-Set Approch<a class="headerlink" href="#module-distil.active_learning_strategies.core_set" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.core_set.CoreSet">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.core_set.</code><code class="sig-name descname">CoreSet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/core_set.html#CoreSet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.core_set.CoreSet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implementation of CoreSet <a class="footnote-reference brackets" href="#sener2018active" id="id7">7</a> Strategy. A diversity-based
approach using coreset selection. The embedding of each example is computed by the network’s
penultimate layer and the samples at each round are selected using a greedy furthest-first
traversal conditioned on all labeled examples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.core_set.CoreSet.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/core_set.html#CoreSet.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.core_set.CoreSet.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.entropy_sampling">
<span id="entropy-sampling"></span><h2>Entropy Sampling<a class="headerlink" href="#module-distil.active_learning_strategies.entropy_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.entropy_sampling.EntropySampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.entropy_sampling.</code><code class="sig-name descname">EntropySampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/entropy_sampling.html#EntropySampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.entropy_sampling.EntropySampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Entropy Sampling Strategy, one of the most basic active learning strategies,
where we select samples about which the model is most uncertain. To quantify the uncertainity
we use entropy and therefore select points which have maximum entropy.
Suppose the model has <cite>nclasses</cite> output nodes and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus,
<span class="math notranslate nohighlight">\(j \in [1,nclasses]\)</span>. Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding
softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Then entropy can be calculated as,</p>
<div class="math notranslate nohighlight">
\[ENTROPY = -\sum_j \sigma(z_j)*\log(\sigma(z_j))\]</div>
<p>The algorithm then selects <cite>budget</cite> no. of elements with highest <strong>ENTROPY</strong>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.entropy_sampling.EntropySampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.entropy_sampling.EntropySampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.entropy_sampling_dropout">
<span id="entropy-sampling-with-dropout"></span><h2>Entropy Sampling with Dropout<a class="headerlink" href="#module-distil.active_learning_strategies.entropy_sampling_dropout" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.entropy_sampling_dropout.EntropySamplingDropout">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.entropy_sampling_dropout.</code><code class="sig-name descname">EntropySamplingDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/entropy_sampling_dropout.html#EntropySamplingDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.entropy_sampling_dropout.EntropySamplingDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Entropy Sampling Strategy with dropout. Entropy Sampling Strategy is one
of the most basic active learning strategies, where we select samples about which the model
is most uncertain. To quantify the uncertainity we use entropy and therefore select points
which have maximum entropy.
Suppose the model has <cite>nclasses</cite> output nodes and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus,
<span class="math notranslate nohighlight">\(j \in [1,nclasses]\)</span>. Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding
softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Then entropy can be calculated as,</p>
<div class="math notranslate nohighlight">
\[ENTROPY = -\sum_j \sigma(z_j)*\log(\sigma(z_j))\]</div>
<p>The algorithm then selects <cite>budget</cite> no. of elements with highest <strong>ENTROPY</strong>.</p>
<p>The drop out version uses the predict probability dropout function from the base strategy class to find the hypothesised labels.
User can pass n_drop argument which denotes the number of times the probabilities will be calculated.
The final probability is calculated by averaging probabilities obtained in all iteraitons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>n_drop</strong>: Number of dropout runs (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.entropy_sampling_dropout.EntropySamplingDropout.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.entropy_sampling_dropout.EntropySamplingDropout.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.fass">
<span id="fass"></span><h2>FASS<a class="headerlink" href="#module-distil.active_learning_strategies.fass" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.fass.FASS">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.fass.</code><code class="sig-name descname">FASS</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/fass.html#FASS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.fass.FASS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implements FASS <a class="footnote-reference brackets" href="#pmlr-v37-wei15" id="id8">8</a> combines the uncertainty sampling
method with a submodular data subset selection framework to label a subset of data points to
train a classifier. Here the based on the ‘top_n’ parameter, ‘top_n*budget’ most uncertain
parameters are filtered. On these filtered points one of the submodular functions viz.
‘facility_location’ , ‘feature_based’, ‘graph_cut’, ‘log_determinant’, ‘disparity_min’, ‘disparity_sum’
is applied to get the final set of points.
We select a subset <span class="math notranslate nohighlight">\(F\)</span> of size <span class="math notranslate nohighlight">\(\beta\)</span> based on uncertainty sampling, such
that <span class="math notranslate nohighlight">\(\beta \ge k\)</span>.</p>
<p>Then select a subset <span class="math notranslate nohighlight">\(S\)</span> by solving</p>
<div class="math notranslate nohighlight">
\[\max \{f(S) \text{ such that } |S| \leq k, S \subseteq F\}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the is the <cite>budget</cite> and <span class="math notranslate nohighlight">\(f\)</span> can be one of these functions -
‘facility_location’ , ‘feature_based’, ‘graph_cut’, ‘log_determinant’, ‘disparity_min’, ‘disparity_sum’.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>submod_args</strong>: Parameters for the submodular selection as described in SubmodularSampling (dict, optional)</p></li>
<li><p><strong>uncertainty_measure</strong>: Describes which measure of uncertainty should be used. This should be one of ‘entropy’, ‘least_confidence’, or ‘margin’ (string, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.fass.FASS.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em>, <em class="sig-param"><span class="n">top_n</span><span class="o">=</span><span class="default_value">5</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/fass.html#FASS.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.fass.FASS.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p></li>
<li><p><strong>top_n</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of slices of size budget to include in filtered subset</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.glister">
<span id="glister"></span><h2>GLISTER<a class="headerlink" href="#module-distil.active_learning_strategies.glister" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.glister.GLISTER">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.glister.</code><code class="sig-name descname">GLISTER</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em>, <em class="sig-param"><span class="n">validation_dataset</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">typeOf</span><span class="o">=</span><span class="default_value">'none'</span></em>, <em class="sig-param"><span class="n">lam</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">kernel_batch_size</span><span class="o">=</span><span class="default_value">200</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/glister.html#GLISTER"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.glister.GLISTER" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This is implementation of GLISTER-ACTIVE from the paper GLISTER: Generalization based Data
Subset Selection for Efficient and Robust Learning <a class="footnote-reference brackets" href="#killamsetty2020glister" id="id9">9</a>. GLISTER
methods tries to solve a bi-level optimisation problem.</p>
<div class="math notranslate nohighlight">
\[\overbrace{\underset{{S \subseteq {\mathcal U}, |S| \leq k}}{\operatorname{argmin\hspace{0.7mm}}} L_V(\underbrace{\underset{\theta}{\operatorname{argmin\hspace{0.7mm}}} L_T( \theta, S)}_{inner-level}, {\mathcal V})}^{outer-level}\]</div>
<p>In the above equation, <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> denotes the Data without lables i.e. <cite>unlabeled_x</cite>,
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span> denotes the validation set that guides the subset selection process, <span class="math notranslate nohighlight">\(L_T\)</span> denotes the
training loss, <span class="math notranslate nohighlight">\(L_V\)</span> denotes the validation loss, <span class="math notranslate nohighlight">\(S\)</span> denotes the data subset selected at each round,  and <span class="math notranslate nohighlight">\(k\)</span> is the <cite>budget</cite>.
Since, solving the complete inner-optimization is expensive, GLISTER-ONLINE adopts a online one-step meta approximation where we approximate the solution to inner problem
by taking a single gradient step.
The optimization problem after the approximation is as follows:</p>
<div class="math notranslate nohighlight">
\[\overbrace{\underset{{S \subseteq {\mathcal U}, |S| \leq k}}{\operatorname{argmin\hspace{0.7mm}}} L_V(\underbrace{\theta - \eta \nabla_{\theta}L_T(\theta, S)}_{inner-level}, {\mathcal V})}^{outer-level}\]</div>
<p>In the above equation, <span class="math notranslate nohighlight">\(\eta\)</span> denotes the step-size used for one-step gradient update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>lr</strong>: The learning rate used for training (float)</p></li>
</ul>
</p></li>
<li><p><strong>validation_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The validation dataset to be used in GLISTER objective</p></li>
<li><p><strong>typeOf</strong> (<em>str</em><em>, </em><em>optional</em>) – Determines the type of regulariser to be used. Default is <strong>‘none’</strong>.
For random regulariser use <strong>‘Rand’</strong>.
To use Facility Location set functiom as a regulariser use <strong>‘FacLoc’</strong>.
To use Diversity set functiom as a regulariser use <strong>‘Diversity’</strong>.</p></li>
<li><p><strong>lam</strong> (<em>float</em><em>, </em><em>optional</em>) – Determines the amount of regularisation to be applied. Mandatory if is not <cite>typeOf=’none’</cite> and by default set to <cite>None</cite>.
For random regulariser use values should be between 0 and 1 as it determines fraction of points replaced by random points.
For both ‘Diversity’ and ‘FacLoc’, <cite>lam</cite> determines the weightage given to them while computing the gain.</p></li>
<li><p><strong>kernel_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – For ‘Diversity’ and ‘FacLoc’ regualrizer versions, similarity kernel is to be computed, which
entails creating a 3d torch tensor of dimenssions kernel_batch_size*kernel_batch_size*
feature dimenssion.Again kernel_batch_size should be such that one can exploit the benefits of
tensorization while honouring the resourse constraits.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.glister.GLISTER.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/glister.html#GLISTER.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.glister.GLISTER.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.gradmatch_active">
<span id="gradmatch"></span><h2>GRADMATCH<a class="headerlink" href="#module-distil.active_learning_strategies.gradmatch_active" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.gradmatch_active.GradMatchActive">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.gradmatch_active.</code><code class="sig-name descname">GradMatchActive</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em>, <em class="sig-param"><span class="n">validation_dataset</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/gradmatch_active.html#GradMatchActive"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.gradmatch_active.GradMatchActive" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This is an implementation of an active learning variant of GradMatch from the paper GRAD-MATCH: A
Gradient Matching Based Data Subset Selection for Efficient Learning <a class="footnote-reference brackets" href="#killamsetty2021grad" id="id10">10</a>.
This algorithm solves a fixed-weight version of the error term present in the paper by a greedy selection
algorithm akin to the original GradMatch’s Orthogonal Matching Pursuit. The gradients computed are on the
hypothesized labels of the loss function and are matched to either the full gradient of these hypothesized
examples or a supplied validation gradient. The indices returned are the ones selected by this algorithm.</p>
<div class="math notranslate nohighlight">
\[Err(X_t, L, L_T, \theta_t) = \left |\left| \sum_{i \in X_t} \nabla_\theta L_T^i (\theta_t) - \frac{k}{N} \nabla_\theta L(\theta_t) \right | \right|\]</div>
<p>where,</p>
<blockquote>
<div><ul class="simple">
<li><p>Each gradient is computed with respect to the last layer’s parameters</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_t\)</span> are the model parameters at selection round <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X_t\)</span> is the queried set of points to label at selection round <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the budget</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of points contributing to the full gradient <span class="math notranslate nohighlight">\(\nabla_\theta L(\theta_t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_\theta L(\theta_t)\)</span> is either the complete hypothesized gradient or a validation gradient</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i \in X_t} \nabla_\theta L_T^i (\theta_t)\)</span> is the subset’s hypothesized gradient with <span class="math notranslate nohighlight">\(|X_t| = k\)</span></p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>grad_embedding</strong>: The type of gradient embedding that should be used (string, optional)</p></li>
<li><p><strong>omp_reg</strong>: The regularization constant to use in GradMatch objective</p></li>
</ul>
</p></li>
<li><p><strong>validation_dataset</strong> (<em>torch.utils.data.Dataset</em><em>, </em><em>optional</em>) – The validation dataset to use in GradMatch objective</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.gradmatch_active.GradMatchActive.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em>, <em class="sig-param"><span class="n">use_weights</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/gradmatch_active.html#GradMatchActive.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.gradmatch_active.GradMatchActive.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p></li>
<li><p><strong>use_weights</strong> (<em>bool</em>) – Whether to use fixed-weight version (false) or OMP version (true)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.kmeans_sampling">
<span id="kmeans-sampling"></span><h2>KMeans Sampling<a class="headerlink" href="#module-distil.active_learning_strategies.kmeans_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.kmeans_sampling.KMeansSampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.kmeans_sampling.</code><code class="sig-name descname">KMeansSampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/kmeans_sampling.html#KMeansSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.kmeans_sampling.KMeansSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implements KMeans Sampling selection strategy, the last layer embeddings are calculated for all the unlabeled data points.
Then the KMeans clustering algorithm is run over these embeddings with the number of clusters equal to the budget.
Then the distance is calculated for all the points from their respective centers. From each cluster, the point closest to
the center is selected to be labeled for the next iteration. Since the number of centers are equal to the budget, selecting
one point from each cluster satisfies the total number of data points to be selected in one iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: Batch size to be used inside strategy class (int, optional)</p></li>
<li><p><strong>device</strong>: The device that this strategy class should use for computation (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss that should be used for relevant computations (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>rand_seed</strong>: Specifies a seed for the random seed generator used in initialization (int, optional)</p></li>
<li><p><strong>representation</strong>: Specifies whether to use the last linear layer embeddings or the raw data. Must be one of ‘linear’ or ‘raw’ (string, optional)</p></li>
<li><p><strong>kmeans_args</strong>: Specifies additional kmeans-related parameters</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>tol</strong>: Specifies the value of the Frobenius norm of the inertia tensor by which kmeans should cease (float, optional)</p></li>
<li><p><strong>max_iter</strong>: Specifies the maximum number of iterations that kmeans should use before terminating (int, optional)</p></li>
<li><p><strong>n_init</strong>: Specifies the number of kmeans run-throughs to use, wherein the one with the smallest inertia is selected for the selection phase (int, optional)</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.kmeans_sampling.KMeansSampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/kmeans_sampling.html#KMeansSampling.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.kmeans_sampling.KMeansSampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.least_confidence_sampling">
<span id="least-confidence"></span><h2>Least Confidence<a class="headerlink" href="#module-distil.active_learning_strategies.least_confidence_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.least_confidence_sampling.LeastConfidenceSampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.least_confidence_sampling.</code><code class="sig-name descname">LeastConfidenceSampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/least_confidence_sampling.html#LeastConfidenceSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.least_confidence_sampling.LeastConfidenceSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Least Confidence Sampling Strategy a active learning strategy where
the algorithm selects the data points for which the model has the lowest confidence while
predicting its label.</p>
<p>Suppose the model has <cite>nclasses</cite> output nodes denoted by <span class="math notranslate nohighlight">\(\overrightarrow{\boldsymbol{z}}\)</span>
and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus, <span class="math notranslate nohighlight">\(j \in [1, nclasses]\)</span>.
Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Then the softmax can be used pick <cite>budget</cite> no. of elements for which the model has the lowest
confidence as follows,</p>
<div class="math notranslate nohighlight">
\[\mbox{argmin}_{{S \subseteq {\mathcal U}, |S| \leq k}}{\sum_S(\mbox{argmax}_j{(\sigma(\overrightarrow{\boldsymbol{z}}))})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> denotes the Data without lables i.e. <cite>unlabeled_x</cite> and <span class="math notranslate nohighlight">\(k\)</span> is the <cite>budget</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.least_confidence_sampling.LeastConfidenceSampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.least_confidence_sampling.LeastConfidenceSampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.least_confidence_sampling_dropout">
<span id="least-confidence-with-dropout"></span><h2>Least Confidence with Dropout<a class="headerlink" href="#module-distil.active_learning_strategies.least_confidence_sampling_dropout" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.least_confidence_sampling_dropout.LeastConfidenceSamplingDropout">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.least_confidence_sampling_dropout.</code><code class="sig-name descname">LeastConfidenceSamplingDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/least_confidence_sampling_dropout.html#LeastConfidenceSamplingDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.least_confidence_sampling_dropout.LeastConfidenceSamplingDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Least Confidence Sampling Strategy with dropout a active learning strategy where
the algorithm selects the data points for which the model has the lowest confidence while
predicting its label.</p>
<p>Suppose the model has <cite>nclasses</cite> output nodes denoted by <span class="math notranslate nohighlight">\(\overrightarrow{\boldsymbol{z}}\)</span>
and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus, <span class="math notranslate nohighlight">\(j \in [1, nclasses]\)</span>.
Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Then the softmax can be used pick <cite>budget</cite> no. of elements for which the model has the lowest
confidence as follows,</p>
<div class="math notranslate nohighlight">
\[\mbox{argmin}_{{S \subseteq {\mathcal U}, |S| \leq k}}{\sum_S(\mbox{argmax}_j{(\sigma(\overrightarrow{\boldsymbol{z}}))})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> denotes the Data without lables i.e. <cite>unlabeled_x</cite> and <span class="math notranslate nohighlight">\(k\)</span> is the <cite>budget</cite>.
The drop out version uses the predict probability dropout function from the base strategy class to find the hypothesised labels.
User can pass n_drop argument which denotes the number of times the probabilities will be calculated.
The final probability is calculated by averaging probabilities obtained in all iteraitons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>n_drop</strong>: Number of dropout runs (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.least_confidence_sampling_dropout.LeastConfidenceSamplingDropout.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.least_confidence_sampling_dropout.LeastConfidenceSamplingDropout.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.margin_sampling">
<span id="margin-sampling"></span><h2>Margin Sampling<a class="headerlink" href="#module-distil.active_learning_strategies.margin_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.margin_sampling.MarginSampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.margin_sampling.</code><code class="sig-name descname">MarginSampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/margin_sampling.html#MarginSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.margin_sampling.MarginSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Margin Sampling Strategy a active learning strategy similar to Least Confidence
Sampling Strategy. While least confidence only takes into consideration the maximum probability,
margin sampling considers the difference between the confidence of first and the second most
probable labels.</p>
<p>Suppose the model has <cite>nclasses</cite> output nodes denoted by <span class="math notranslate nohighlight">\(\overrightarrow{\boldsymbol{z}}\)</span>
and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus, <span class="math notranslate nohighlight">\(j \in [1, nclasses]\)</span>.
Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Let,</p>
<div class="math notranslate nohighlight">
\[m = \mbox{argmax}_j{(\sigma(\overrightarrow{\boldsymbol{z}}))}\]</div>
<p>Then using softmax, Margin Sampling Strategy would pick <cite>budget</cite> no. of elements as follows,</p>
<div class="math notranslate nohighlight">
\[\mbox{argmin}_{{S \subseteq {\mathcal U}, |S| \leq k}}{\sum_S(\mbox{argmax}_j {(\sigma(\overrightarrow{\boldsymbol{z}}))}) - (\mbox{argmax}_{j \ne m} {(\sigma(\overrightarrow{\boldsymbol{z}}))})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> denotes the Data without lables i.e. <cite>unlabeled_x</cite> and <span class="math notranslate nohighlight">\(k\)</span> is the <cite>budget</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.margin_sampling.MarginSampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.margin_sampling.MarginSampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.margin_sampling_dropout">
<span id="margin-sampling-with-dropout"></span><h2>Margin Sampling with Dropout<a class="headerlink" href="#module-distil.active_learning_strategies.margin_sampling_dropout" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.margin_sampling_dropout.MarginSamplingDropout">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.margin_sampling_dropout.</code><code class="sig-name descname">MarginSamplingDropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/margin_sampling_dropout.html#MarginSamplingDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.margin_sampling_dropout.MarginSamplingDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.score_streaming_strategy.ScoreStreamingStrategy</span></code></p>
<p>Implements the Margin Sampling Strategy with dropout a active learning strategy similar to Least Confidence
Sampling Strategy with dropout. While least confidence only takes into consideration the maximum probability,
margin sampling considers the difference between the confidence of first and the second most
probable labels.</p>
<p>Suppose the model has <cite>nclasses</cite> output nodes denoted by <span class="math notranslate nohighlight">\(\overrightarrow{\boldsymbol{z}}\)</span>
and each output node is denoted by <span class="math notranslate nohighlight">\(z_j\)</span>. Thus, <span class="math notranslate nohighlight">\(j \in [1, nclasses]\)</span>.
Then for a output node <span class="math notranslate nohighlight">\(z_i\)</span> from the model, the corresponding softmax would be</p>
<div class="math notranslate nohighlight">
\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</div>
<p>Let,</p>
<div class="math notranslate nohighlight">
\[m = \mbox{argmax}_j{(\sigma(\overrightarrow{\boldsymbol{z}}))}\]</div>
<p>Then using softmax, Margin Sampling Strategy would pick <cite>budget</cite> no. of elements as follows,</p>
<div class="math notranslate nohighlight">
\[\mbox{argmin}_{{S \subseteq {\mathcal U}, |S| \leq k}}{\sum_S(\mbox{argmax}_j {(\sigma(\overrightarrow{\boldsymbol{z}}))}) - (\mbox{argmax}_{j \ne m} {(\sigma(\overrightarrow{\boldsymbol{z}}))})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> denotes the Data without lables i.e. <cite>unlabeled_x</cite> and <span class="math notranslate nohighlight">\(k\)</span> is the <cite>budget</cite>.</p>
<p>The drop out version uses the predict probability dropout function from the base strategy class to find the hypothesised labels.
User can pass n_drop argument which denotes the number of times the probabilities will be calculated.
The final probability is calculated by averaging probabilities obtained in all iteraitons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>n_drop</strong>: Number of dropout runs (int, optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.margin_sampling_dropout.MarginSamplingDropout.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="headerlink" href="#distil.active_learning_strategies.margin_sampling_dropout.MarginSamplingDropout.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.partition_strategy">
<span id="partitioning"></span><h2>Partitioning<a class="headerlink" href="#module-distil.active_learning_strategies.partition_strategy" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.partition_strategy.PartitionStrategy">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.partition_strategy.</code><code class="sig-name descname">PartitionStrategy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em>, <em class="sig-param"><span class="n">query_dataset</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">private_dataset</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/partition_strategy.html#PartitionStrategy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.partition_strategy.PartitionStrategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Provides a wrapper around most of the strategies implemented in DISTIL that allows one to select portions of the budget from
specific partitions of the unlabeled dataset. This allows the use of some strategies that would otherwise fail due to time or memory
constraints. For example, if one specifies a number of partitions to be 5 and wants to select 50 new points, 10 points would
be selected from the first fifth of the dataset, 10 points would be selected from the second fifth of the dataset, and so on.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>num_partitions</strong>: Number of partitons to use (int, optional)</p></li>
<li><p><strong>wrapped_strategy_class</strong>: The class of the strategy to use (class, optional)</p></li>
</ul>
</p></li>
<li><p><strong>query_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The query dataset to use if the wrapped_strategy_class argument points to SMI or SCMI.</p></li>
<li><p><strong>private_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The private dataset to use if the wrapped_strategy_class argument points to SCG or SCMI.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.partition_strategy.PartitionStrategy.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/partition_strategy.html#PartitionStrategy.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.partition_strategy.PartitionStrategy.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.random_sampling">
<span id="random-sampling"></span><h2>Random Sampling<a class="headerlink" href="#module-distil.active_learning_strategies.random_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.random_sampling.RandomSampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.random_sampling.</code><code class="sig-name descname">RandomSampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/random_sampling.html#RandomSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.random_sampling.RandomSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>Implementation of Random Sampling Strategy. This strategy is often used as a baseline,
where we pick a set of unlabeled points randomly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.random_sampling.RandomSampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/random_sampling.html#RandomSampling.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.random_sampling.RandomSampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.scg">
<span id="submodular-conditional-gain-scg"></span><h2>Submodular Conditional Gain (SCG)<a class="headerlink" href="#module-distil.active_learning_strategies.scg" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.scg.SCG">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.scg.</code><code class="sig-name descname">SCG</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">private_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/scg.html#SCG"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.scg.SCG" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Conditional Gain (SCG) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id11">11</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular conditional gain
between this set of points and a provided private set is maximized. Doing so allows a practitioner to select
points from an unlabeled set that are dissimilar to points provided in the private set.</p>
<p>These submodular conditional gain functions rely on formulating embeddings for the points in the unlabeled set
and the private set. Once these embeddings are formed, similarity kernels are formed from these
embeddings based on a similarity metric. Once these similarity kernels are formed, they are used in computing the value
of each submodular conditional gain function. Hence, common techniques for submodular maximization subject to a
cardinality constraint can be used, such as the naive greedy algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, three submodular conditional gain functions are implemented: ‘flcg’, ‘gccg’, and ‘logdetcg’. Each
function is obtained by applying the definition of a submodular conditional gain function using common
submodular functions. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id12">12</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>private_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The private dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the private dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</p>
<blockquote>
<div><ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>scg_function</strong>: The submodular conditional gain function to use in optimization. Must be one of ‘flcg’, ‘gccg’, or ‘logdetcg’.  (string)</p></li>
<li><p><strong>optimizer</strong>: The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’, ‘LazyGreedy’ and ‘LazierThanLazyGreedy’. (string, optional)</p></li>
<li><p><strong>metric</strong>: The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’. (string)</p></li>
<li><p><strong>nu</strong>: A parameter that governs the hardness of the privacy constraint. (float)</p></li>
<li><p><strong>embedding_type</strong>: The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or ‘features’. (string)</p></li>
<li><p><strong>gradType</strong>: When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the loss function using both. (string)</p></li>
<li><p><strong>layer_name</strong>: When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of the model. (string)</p></li>
<li><p><strong>stopIfZeroGain</strong>: Controls if the optimizer should cease maximization if there is zero gain in the submodular objective. (bool)</p></li>
<li><p><strong>stopIfNegativeGain</strong>: Controls if the optimizer should cease maximization if there is negative gain in the submodular objective. (bool)</p></li>
<li><p><strong>verbose</strong>: Gives a more verbose output when calling select() when True. (bool)</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.scg.SCG.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/scg.html#SCG.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.scg.SCG.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.scmi">
<span id="submodular-conditional-mutual-information-scmi"></span><h2>Submodular Conditional Mutual Information (SCMI)<a class="headerlink" href="#module-distil.active_learning_strategies.scmi" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.scmi.SCMI">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.scmi.</code><code class="sig-name descname">SCMI</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">query_dataset</span></em>, <em class="sig-param"><span class="n">private_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/scmi.html#SCMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.scmi.SCMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Conditional Mutual Information (SCMI) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id13">11</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular conditional mutual information
between this set of points and a provided query set is maximized, conditioned on a private dataset.
Doing so allows a practitioner to select points from an unlabeled set that are SIMILAR to points that they have
provided in the query set while being dissimilar to points provided in the private set.</p>
<p>These submodular conditional mutual information functions rely on formulating embeddings for the points in the query set,
the unlabeled set, and the private set. Once these embeddings are formed, similarity kernels are formed from these
embeddings based on a similarity metric. Once these similarity kernels are formed, they are used in computing the value
of each submodular conditional mutual information function. Hence, common techniques for submodular maximization
subject to a cardinality constraint can be used, such as the naive greedy algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, two submodular conditional mutual information functions are implemented: ‘flcmi’ and ‘logdetcmi’. Each
function is obtained by applying the definition of a submodular conditional mutual information function using common
submodular functions. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id14">12</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>query_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The query dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the query dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>private_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The private dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the private dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</p>
<blockquote>
<div><ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>scmi_function</strong>: The submodular conditional mutual information function to use in optimization. Must be one of ‘flcmi’ or ‘logdetcmi’.  (string)</p></li>
<li><p><strong>optimizer</strong>: The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’, ‘LazyGreedy’ and ‘LazierThanLazyGreedy’. (string, optional)</p></li>
<li><p><strong>metric</strong>: The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’. (string)</p></li>
<li><p><strong>eta</strong>: A magnification constant that is used in all but gcmi. It is used as a value of query-relevance vs diversity trade-off. Increasing eta tends to increase query-relevance while reducing query-coverage and diversity. (float)</p></li>
<li><p><strong>nu</strong>: A parameter that governs the hardness of the privacy constraint. (float)</p></li>
<li><p><strong>embedding_type</strong>: The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or ‘features’. (string)</p></li>
<li><p><strong>gradType</strong>: When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the loss function using both. (string)</p></li>
<li><p><strong>layer_name</strong>: When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of the model. (string)</p></li>
<li><p><strong>stopIfZeroGain</strong>: Controls if the optimizer should cease maximization if there is zero gain in the submodular objective. (bool)</p></li>
<li><p><strong>stopIfNegativeGain</strong>: Controls if the optimizer should cease maximization if there is negative gain in the submodular objective. (bool)</p></li>
<li><p><strong>verbose</strong>: Gives a more verbose output when calling select() when True. (bool)</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.scmi.SCMI.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/scmi.html#SCMI.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.scmi.SCMI.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.smi">
<span id="submodular-mutual-information-smi"></span><h2>Submodular Mutual Information (SMI)<a class="headerlink" href="#module-distil.active_learning_strategies.smi" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.smi.SMI">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.smi.</code><code class="sig-name descname">SMI</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">query_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/smi.html#SMI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.smi.SMI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This strategy implements the Submodular Mutual Information (SMI) selection paradigm discuss in the paper
SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios <a class="footnote-reference brackets" href="#kothawade2021similar" id="id15">11</a>. In this selection
paradigm, points from the unlabeled dataset are chosen in such a way that the submodular mutual information
between this set of points and a provided query set is maximized. Doing so allows a practitioner to select
points from an unlabeled set that are SIMILAR to points that they have provided in a active learning query.</p>
<p>These submodular mutual information functions rely on formulating embeddings for the points in the query set
and the unlabeled set. Once these embeddings are formed, one or more similarity kernels (depending on the
SMI function used) are formed from these embeddings based on a similarity metric. Once these similarity kernels
are formed, they are used in computing the value of each submodular mutual information function. Hence, common
techniques for submodular maximization subject to a cardinality constraint can be used, such as the naive greedy
algorithm, the lazy greedy algorithm, and so forth.</p>
<p>In this framework, we set the cardinality constraint to be the active learning selection budget; hence, a list of
indices with a total length less than or equal to this cardinality constraint will be returned. Depending on the
maximization configuration, one can ensure that the length of this list will be equal to the cardinality constraint.</p>
<p>Currently, five submodular mutual information functions are implemented: fl1mi, fl2mi, gcmi, logdetmi, and com. Each
function is obtained by applying the definition of a submodular mutual information function using common submodular
functions. Facility Location Mutual Information (fl1mi) models pairwise similarities of points in the query set to
points in the unlabeled dataset AND pairwise similarities of points within the unlabeled datasets. Another variant of
Facility Location Mutual Information (fl2mi) models pairwise similarities of points in the query set to points in
the unlabeled dataset ONLY. Graph Cut Mutual Information (gcmi), Log-Determinant Mutual Information (logdetmi), and
Concave-Over-Modular Mutual Information (com) are all obtained by applying the usual submodular function under this
definition. For more information-theoretic discussion, consider referring to the paper Submodular Combinatorial
Information Measures with Applications in Machine Learning <a class="footnote-reference brackets" href="#iyer2021submodular" id="id16">12</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled dataset to be used in this strategy. For the purposes of selection, the labeled dataset is not used,
but it is provided to fit the common framework of the Strategy superclass.</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled dataset to be used in this strategy. It is used in the selection process as described above.
Importantly, the unlabeled dataset must return only a data Tensor; if indexing the unlabeled dataset returns a tuple of
more than one component, unexpected behavior will most likely occur.</p></li>
<li><p><strong>query_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The query dataset to be used in this strategy. It is used in the selection process as described above. Notably,
the query dataset should be labeled; hence, indexing the query dataset should return a data/label pair. This is
done in this fashion to allow for gradient embeddings.</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The neural network model to use for embeddings and predictions. Notably, all embeddings typically come from extracted
features from this network or from gradient embeddings based on the loss, which can be based on hypothesized gradients
or on true gradients (depending on the availability of the label).</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – The number of classes being predicted by the neural network.</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>A dictionary containing many configurable settings for this strategy. Each key-value pair is described below:</p>
<blockquote>
<div><ul>
<li><p><strong>batch_size</strong>: The batch size used internally for torch.utils.data.DataLoader objects. (int, optional)</p></li>
<li><p><strong>device</strong>: The device to be used for computation. PyTorch constructs are transferred to this device. Usually is one of ‘cuda’ or ‘cpu’. (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss function to be used in computations. (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>smi_function</strong>: The submodular mutual information function to use in optimization. Must be one of ‘fl1mi’, ‘fl2mi’, ‘gcmi’, ‘logdetmi’, ‘com’. (string)</p></li>
<li><p><strong>optimizer</strong>: The optimizer to use for submodular maximization. Can be one of ‘NaiveGreedy’, ‘StochasticGreedy’, ‘LazyGreedy’ and ‘LazierThanLazyGreedy’. (string, optional)</p></li>
<li><p><strong>metric</strong>: The similarity metric to use for similarity kernel computation. This can be either ‘cosine’ or ‘euclidean’. (string)</p></li>
<li><p><strong>eta</strong>: A magnification constant that is used in all but gcmi. It is used as a value of query-relevance vs diversity trade-off. Increasing eta tends to increase query-relevance while reducing query-coverage and diversity. (float)</p></li>
<li><p><strong>embedding_type</strong>: The type of embedding to compute for similarity kernel computation. This can be either ‘gradients’ or ‘features’. (string)</p></li>
<li><p><strong>gradType</strong>: When ‘embedding_type’ is ‘gradients’, this defines the type of gradient to use. ‘bias’ creates gradients from the loss function with respect to the biases outputted by the model. ‘linear’ creates gradients from the loss function with respect to the last linear layer features. ‘bias_linear’ creates gradients from the loss function using both. (string)</p></li>
<li><p><strong>layer_name</strong>: When ‘embedding_type’ is ‘features’, this defines the layer within the neural network that is used to extract feature embeddings. Namely, this argument must be the name of a module used in the forward() computation of the model. (string)</p></li>
<li><p><strong>stopIfZeroGain</strong>: Controls if the optimizer should cease maximization if there is zero gain in the submodular objective. (bool)</p></li>
<li><p><strong>stopIfNegativeGain</strong>: Controls if the optimizer should cease maximization if there is negative gain in the submodular objective. (bool)</p></li>
<li><p><strong>verbose</strong>: Gives a more verbose output when calling select() when True. (bool)</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.smi.SMI.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/smi.html#SMI.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.smi.SMI.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-distil.active_learning_strategies.submod_sampling">
<span id="submodular-sampling"></span><h2>Submodular Sampling<a class="headerlink" href="#module-distil.active_learning_strategies.submod_sampling" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="distil.active_learning_strategies.submod_sampling.SubmodularSampling">
<em class="property">class </em><code class="sig-prename descclassname">distil.active_learning_strategies.submod_sampling.</code><code class="sig-name descname">SubmodularSampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">labeled_dataset</span></em>, <em class="sig-param"><span class="n">unlabeled_dataset</span></em>, <em class="sig-param"><span class="n">net</span></em>, <em class="sig-param"><span class="n">nclasses</span></em>, <em class="sig-param"><span class="n">args</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/submod_sampling.html#SubmodularSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.submod_sampling.SubmodularSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">distil.active_learning_strategies.strategy.Strategy</span></code></p>
<p>This strategy uses one of the submodular functions viz. ‘facility_location’, ‘feature_based’, ‘graph_cut’,
‘log_determinant’, ‘disparity_min’, or ‘disparity_sum’ <a class="footnote-reference brackets" href="#iyer2021submodular" id="id17">12</a>, <a class="footnote-reference brackets" href="#dasgupta-etal-2013-summarization" id="id18">13</a>
to select new points via submodular maximization. These techniques can be applied directly to the features/embeddings
or on the gradients of the loss functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The labeled training dataset</p></li>
<li><p><strong>unlabeled_dataset</strong> (<em>torch.utils.data.Dataset</em>) – The unlabeled pool dataset</p></li>
<li><p><strong>net</strong> (<em>torch.nn.Module</em>) – The deep model to use</p></li>
<li><p><strong>nclasses</strong> (<em>int</em>) – Number of unique values for the target</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>Specify additional parameters</p>
<ul>
<li><p><strong>batch_size</strong>: Batch size to be used inside strategy class (int, optional)</p></li>
<li><p><strong>device</strong>: The device that this strategy class should use for computation (string, optional)</p></li>
<li><p><strong>loss</strong>: The loss that should be used for relevant computations (typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional)</p></li>
<li><p><strong>submod_args</strong>: Additional parameters for submodular selection (dict, optional)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>submod</strong>: The choice of submodular function to use. Must be one of ‘facility_location’, ‘feature_based’, ‘graph_cut’, ‘log_determinant’, ‘disparity_min’, ‘disparity_sum’ (string)</p></li>
<li><p><strong>metric</strong>: The similarity metric to use in relevant functions. Must be one of ‘cosine’ or ‘euclidean’ (string)</p></li>
<li><p><strong>representation</strong>: The representation of each data point to be used in submodular selection. Must be one of ‘linear’, ‘grad_bias’, ‘grad_linear’, ‘grad_bias_linear’ (string)</p></li>
<li><p><strong>feature_weights</strong>: If using ‘feature_based’, then this specifies the weights for each feature (list)</p></li>
<li><p><strong>concave_function</strong>: If using ‘feature_based’, then this specifies the concave function to apply in the feature-based objective (typing.Callable)</p></li>
<li><p><strong>lambda_val</strong>: If using ‘graph_cut’ or ‘log_determinant’, then this specifies the lambda constant to be used in both functions (float)</p></li>
<li><p><strong>optimizer</strong>: The choice of submodular optimization technique to use. Must be one of ‘NaiveGreedy’, ‘StochasticGreedy’, ‘LazyGreedy’, or ‘LazierThanLazyGreedy’ (string)</p></li>
<li><p><strong>stopIfZeroGain</strong>: Whether to stop if adding a point results in zero gain in the submodular objective function (bool)</p></li>
<li><p><strong>stopIfNegativeGain</strong>: Whether to stop if adding a point results in negative gain in the submodular objective function (bool)</p></li>
<li><p><strong>verbose</strong>: Whether to print more verbose output</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="distil.active_learning_strategies.submod_sampling.SubmodularSampling.select">
<code class="sig-name descname">select</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">budget</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/distil/active_learning_strategies/submod_sampling.html#SubmodularSampling.select"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#distil.active_learning_strategies.submod_sampling.SubmodularSampling.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects next set of points</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>budget</strong> (<em>int</em>) – Number of data points to select for labeling</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>idxs</strong> – List of selected data point indices with respect to unlabeled_dataset</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="references">
<h2>REFERENCES<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><dl class="footnote brackets">
<dt class="label" id="tramer2017ensemble"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: attacks and defenses. <em>arXiv preprint arXiv:1705.07204</em>, 2017.</p>
</dd>
<dt class="label" id="ducoffe2018adversarial"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based approach. 2018. <a class="reference external" href="https://arxiv.org/abs/1802.09841">arXiv:1802.09841</a>.</p>
</dd>
<dt class="label" id="moosavi-dezfooli-2016-cvpr"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. June 2016.</p>
</dd>
<dt class="label" id="dblp-badge"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1906.03671">http://arxiv.org/abs/1906.03671</a>, <a class="reference external" href="https://arxiv.org/abs/1906.03671">arXiv:1906.03671</a>.</p>
</dd>
<dt class="label" id="kirsch2019batchbald"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: efficient and diverse batch acquisition for deep bayesian active learning. 2019. <a class="reference external" href="https://arxiv.org/abs/1906.08158">arXiv:1906.08158</a>.</p>
</dd>
<dt class="label" id="houlsby2011bayesian"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification and preference learning. 2011. <a class="reference external" href="https://arxiv.org/abs/1112.5745">arXiv:1112.5745</a>.</p>
</dd>
<dt class="label" id="sener2018active"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: a core-set approach. 2018. <a class="reference external" href="https://arxiv.org/abs/1708.00489">arXiv:1708.00489</a>.</p>
</dd>
<dt class="label" id="pmlr-v37-wei15"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In Francis Bach and David Blei, editors, <em>Proceedings of the 32nd International Conference on Machine Learning</em>, volume 37 of Proceedings of Machine Learning Research, 1954–1963. Lille, France, 07–09 Jul 2015. PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v37/wei15.html">http://proceedings.mlr.press/v37/wei15.html</a>.</p>
</dd>
<dt class="label" id="killamsetty2020glister"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: generalization based data subset selection for efficient and robust learning. 2020. <a class="reference external" href="https://arxiv.org/abs/2012.10630">arXiv:2012.10630</a>.</p>
</dd>
<dt class="label" id="killamsetty2021grad"><span class="brackets"><a class="fn-backref" href="#id10">10</a></span></dt>
<dd><p>Krishnateja Killamsetty, Durga Sivasubramanian, Baharan Mirzasoleiman, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: a gradient matching based data subset selection for efficient learning. <em>arXiv preprint arXiv:2103.00123</em>, 2021.</p>
</dd>
<dt class="label" id="kothawade2021similar"><span class="brackets">11</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id13">2</a>,<a href="#id15">3</a>)</span></dt>
<dd><p>Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: submodular information measures based active learning in realistic scenarios. <em>arXiv preprint arXiv:2107.00717</em>, 2021.</p>
</dd>
<dt class="label" id="iyer2021submodular"><span class="brackets">12</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id14">2</a>,<a href="#id16">3</a>,<a href="#id17">4</a>)</span></dt>
<dd><p>Rishabh Iyer, Ninad Khargonkar, Jeff Bilmes, and Himanshu Asnani. Submodular combinatorial information measures with applications in machine learning. 2021. <a class="reference external" href="https://arxiv.org/abs/2006.15412">arXiv:2006.15412</a>.</p>
</dd>
<dt class="label" id="dasgupta-etal-2013-summarization"><span class="brackets"><a class="fn-backref" href="#id18">13</a></span></dt>
<dd><p>Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. Summarization through submodularity and dispersion. In <em>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1014–1022. Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL: <a class="reference external" href="https://www.aclweb.org/anthology/P13-1100">https://www.aclweb.org/anthology/P13-1100</a>.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="distil.utils.html" class="btn btn-neutral float-right" title="Utilities" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="modules.html" class="btn btn-neutral" title="DISTIL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Durga Sivasubramanian, Nathan Beck, Apurva Dani, Rishabh Iyer.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'v0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/language_data.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>