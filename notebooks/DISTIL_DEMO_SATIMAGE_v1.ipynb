{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DISTIL_DEMO_SATIMAGE_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4vafrlTGa9-"
      },
      "source": [
        "# **DISTIL Installation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZnCocrUGgYD"
      },
      "source": [
        "!git clone https://github.com/decile-team/distil.git\n",
        "!git clone https://github.com/decile-team/datasets.git\n",
        "!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ submodlib\n",
        "%cd /content/distil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPSV3mJ0Gjls"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS8dVJqpGjy0"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "\n",
        "from distil.active_learning_strategies import GLISTER, BADGE, EntropySampling, RandomSampling, SubmodularSampling\n",
        "from distil.utils.models.simple_net import TwoLayerNet\n",
        "from distil.utils.train_helper import data_train\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfv3T-zBGp6P"
      },
      "source": [
        "# **Data, Model & Directory Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seu_jbfIGvIq"
      },
      "source": [
        "def libsvm_file_load(path,dim, save_data=False):\n",
        "    data = []\n",
        "    target = []\n",
        "    with open(path) as fp:\n",
        "       line = fp.readline()\n",
        "       while line:\n",
        "        temp = [i for i in line.strip().split(\" \")]\n",
        "        target.append(int(float(temp[0]))) # Class Number. # Not assumed to be in (0, K-1)\n",
        "        temp_data = [0]*dim\n",
        "        \n",
        "        for i in temp[1:]:\n",
        "            ind,val = i.split(':')\n",
        "            temp_data[int(ind)-1] = float(val)\n",
        "        data.append(temp_data)\n",
        "        line = fp.readline()\n",
        "    X_data = np.array(data,dtype=np.float32)\n",
        "    Y_label = np.array(target)\n",
        "    if save_data:\n",
        "        # Save the numpy files to the folder where they come from\n",
        "        data_np_path = path + '.data.npy'\n",
        "        target_np_path = path + '.label.npy'\n",
        "        np.save(data_np_path, X_data)\n",
        "        np.save(target_np_path, Y_label)\n",
        "    return (X_data, Y_label)\n",
        "\n",
        "    \n",
        "trn_file = '../datasets/satimage/satimage.scale.trn'\n",
        "val_file = '../datasets/satimage/satimage.scale.val'\n",
        "tst_file = '../datasets/satimage/satimage.scale.tst'\n",
        "\n",
        "data_dims = 36\n",
        "nclasses = 6\n",
        "\n",
        "x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)\n",
        "x_val, y_val = libsvm_file_load(val_file, dim=data_dims)\n",
        "x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)\n",
        "\n",
        "y_trn -= 1  # First Class should be zero\n",
        "y_val -= 1\n",
        "y_tst -= 1  # First Class should be zero\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_trn = sc.fit_transform(x_trn)\n",
        "x_val = sc.transform(x_val)\n",
        "x_tst = sc.transform(x_tst)\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6qg8oIWG0g8"
      },
      "source": [
        "X_unlabeled = deepcopy(x_trn)\n",
        "y_unlabeled = deepcopy(y_trn)\n",
        "X_test = deepcopy(x_tst)\n",
        "y_test = deepcopy(y_tst)\n",
        "\n",
        "nSamps, dim = np.shape(X_unlabeled)\n",
        "\n",
        "np.random.seed(42)\n",
        "start_idxs = np.random.choice(nSamps, size=int(0.01*nSamps), replace=False)\n",
        "\n",
        "X_tr = X_unlabeled[start_idxs]\n",
        "X_unlabeled = np.delete(X_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "y_tr = y_unlabeled[start_idxs]\n",
        "y_unlabeled = np.delete(y_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(x_tst), torch.tensor(y_tst, dtype=torch.long))\n",
        "\n",
        "n_rounds = 10    ##Number of rounds to run ac\n",
        "budget = int(0.01*x_trn.shape[0]) \n",
        "\n",
        "net = TwoLayerNet(data_dims, nclasses, 40)\n",
        "net.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn13lujmHHqv"
      },
      "source": [
        "#Model Directory\n",
        "base_dir = \"/content/satimage/\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTyDOxTAHRQl"
      },
      "source": [
        "# **Initial Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHuxnhOzHNws"
      },
      "source": [
        "args = {'n_epoch':500, 'lr':float(0.01),'batch_size':16, 'max_accuracy':0.99, 'window_size':20} \n",
        "dt = data_train(training_dataset, net, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaZS0_O3Hr8A"
      },
      "source": [
        "# **Load Base Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmos4kJnHrfQ"
      },
      "source": [
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBkdgCrYIR3v"
      },
      "source": [
        "# **Random**\n",
        "This strategy is often used as a baseline, where we pick a set of unlabled points randomly. Here we create a instance of distil.active_learning_strategies.random_sampling.RandomSampling by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a wrapper around it that strips the label\n",
        "\n",
        "**net (class object)** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar.\n",
        "\n",
        "**nclasses (int)** – No. of classes in tha dataset\n",
        "\n",
        "**args (dictionary)**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXPxaYf0ISJN"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 16, 'lr':float(0.01)}\n",
        "strategy = RandomSampling(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':500, 'lr':float(0.01),'batch_size':16, 'max_accuracy':0.99, 'window_size':20} \n",
        "dt = data_train(training_dataset, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(test_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    #Adding new points to training set\n",
        "    X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n",
        "    X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n",
        "\n",
        "    #Human In Loop, Assuming user adds new labels here\n",
        "    y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n",
        "    y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n",
        "    print('Number of training points -',X_tr.shape[0])\n",
        "\n",
        "    training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "    unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "\n",
        "    #Reload state and start training\n",
        "    strategy.update_data(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n",
        "    dt.update_data(training_dataset)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(test_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBpuErWxAHCc"
      },
      "source": [
        "# **Uncertanity based Active learning Strategy**\n",
        "\n",
        "The most basic active learning strategy, where we select samples about which the model is most uncertain. To quantify the uncertainity we use entropy, therefore select points which have maximum entropy. Let $z_i$ be output from the model then the correponding softmax would be $$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$. Then entropy can be calculated as, $$ENTROPY = -\\sum_j \\sigma(z_j)*log(\\sigma(z_j))$$\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.entropy_sampling.EntropySampling with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaknF2AU3OXN"
      },
      "source": [
        "**Reinitialize Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "47e7oPOGk0CW"
      },
      "source": [
        "X_unlabeled = deepcopy(x_trn)\n",
        "y_unlabeled = deepcopy(y_trn)\n",
        "X_test = deepcopy(x_tst)\n",
        "y_test = deepcopy(y_tst)\n",
        "\n",
        "nSamps, dim = np.shape(X_unlabeled)\n",
        "\n",
        "np.random.seed(42)\n",
        "start_idxs = np.random.choice(nSamps, size=int(0.01*nSamps), replace=False)\n",
        "\n",
        "X_tr = X_unlabeled[start_idxs]\n",
        "X_unlabeled = np.delete(X_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "y_tr = y_unlabeled[start_idxs]\n",
        "y_unlabeled = np.delete(y_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(x_tst), torch.tensor(y_tst, dtype=torch.long))\n",
        "\n",
        "n_rounds = 10    ##Number of rounds to run ac\n",
        "budget = int(0.01*x_trn.shape[0]) \n",
        "\n",
        "net = TwoLayerNet(data_dims, nclasses, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NX_MRCNpELT"
      },
      "source": [
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhBSD6JQk0CW"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 16, 'lr':float(0.01)}\n",
        "strategy = EntropySampling(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':500, 'lr':float(0.01),'batch_size':16, 'max_accuracy':0.99, 'window_size':20} \n",
        "dt = data_train(training_dataset, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(test_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    #Adding new points to training set\n",
        "    X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n",
        "    X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n",
        "\n",
        "    #Human In Loop, Assuming user adds new labels here\n",
        "    y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n",
        "    y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n",
        "    print('Number of training points -',X_tr.shape[0])\n",
        "\n",
        "    training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "    unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "\n",
        "    #Reload state and start training\n",
        "    strategy.update_data(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n",
        "    dt.update_data(training_dataset)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(test_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39teVabl9Kwa"
      },
      "source": [
        "# **BADGE**\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). Here at each around of selection loss gradients are computed using the hypothesised lables. Then to points to be labled are selected by applying k-means++ on these loss gradients. \n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.badge.BADGE with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV5_7SLRAboA"
      },
      "source": [
        "**Reinitialize Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DL3HAaEyk0CX"
      },
      "source": [
        "X_unlabeled = deepcopy(x_trn)\n",
        "y_unlabeled = deepcopy(y_trn)\n",
        "X_test = deepcopy(x_tst)\n",
        "y_test = deepcopy(y_tst)\n",
        "\n",
        "nSamps, dim = np.shape(X_unlabeled)\n",
        "\n",
        "np.random.seed(42)\n",
        "start_idxs = np.random.choice(nSamps, size=int(0.01*nSamps), replace=False)\n",
        "\n",
        "X_tr = X_unlabeled[start_idxs]\n",
        "X_unlabeled = np.delete(X_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "y_tr = y_unlabeled[start_idxs]\n",
        "y_unlabeled = np.delete(y_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(x_tst), torch.tensor(y_tst, dtype=torch.long))\n",
        "\n",
        "n_rounds = 10    ##Number of rounds to run ac\n",
        "budget = int(0.01*x_trn.shape[0]) \n",
        "\n",
        "net = TwoLayerNet(data_dims, nclasses, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hgmc_dwpMI1"
      },
      "source": [
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ui4OSiufk0CX"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 16, 'lr':float(0.01)}\n",
        "strategy = BADGE(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':500, 'lr':float(0.01),'batch_size':16, 'max_accuracy':0.99, 'window_size':20} \n",
        "dt = data_train(training_dataset, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(test_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    #Adding new points to training set\n",
        "    X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n",
        "    X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n",
        "\n",
        "    #Human In Loop, Assuming user adds new labels here\n",
        "    y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n",
        "    y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n",
        "    print('Number of training points -',X_tr.shape[0])\n",
        "\n",
        "    training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "    unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "\n",
        "    #Reload state and start training\n",
        "    strategy.update_data(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n",
        "    dt.update_data(training_dataset)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(test_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onXfASME-IM7"
      },
      "source": [
        "# **GLISTER**\n",
        "This is implemetation of GLISTER-ACTIVE from the paper [GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning](https://arxiv.org/abs/2012.10630). GLISTER methods tries to solve a bi-level optimisation problem.\n",
        "\\begin{equation*}\n",
        "\\overbrace{\\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmax\\hspace{0.7mm}}} LL_V(\\underbrace{\\underset{\\theta}{\\operatorname{argmax\\hspace{0.7mm}}} LL_T( \\theta, S)}_{inner-level}, {\\mathcal V})}^{outer-level}\n",
        "\\end{equation*}\n",
        "where is $S$ is set of points selected at each round,${\\mathcal V}$ could be a dedicated validation set with labled points or could be union of labeled and unlabeled points with hypothesised labels, $k$ is the budget.\n",
        "To set ${\\mathcal V}$ to be validation set, while calling **GLISTER** class in the toolkit set _valid=TRUE_ and pass validation set otherwise set _valid=False_.\n",
        "\n",
        "Solving this problem directly is almost impossible, therefore we resort to one-step approxiations.We start we $S^0$ as empty set and bulid it as $S^k = S^{k-1} \\cup e$, where $e$ is $\\underset{e}{\\operatorname{argmax\\hspace{0.7mm}}} G_{\\theta}(e | S^k)$. We define,$$G_{\\theta}(e | S^k) = LL_{V}(\\theta^{k}, {\\mathcal V})$$ and update $$\\theta^k \\leftarrow \\theta^{k-1} -  \\eta \\nabla_{\\theta} LL_T(\\hat{\\theta}, e)$$ where $\\hat{\\theta}$ is the parameters of the model at the begining of the selection.\n",
        "To prevent overfitting, we can add regularizer to GLISTER, which can be set by **_typeOf_**. **_typeOf_** can be set to - **'none'**(which is default) for normal GLISTER,**'Rand'** for replacing **_lam_** fraction of points replaced by random points, **'Diversity'** adding diversity set function while computing gain and **'FacLoc'** adding Facility Location set function while computing gain. **_lam_** for both **'Diversity'** and **'FacLoc'** determines the weightage given to them while computing the gain.\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.glister.GLISTER( with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling, we slight change that, **args** dictionary should have keys ‘batch_size’ and ‘lr’. ‘lr’ should be the learning rate used for training. In addition to those folowing additional parameters may be passed:\n",
        "\n",
        "**validation_dataset (torch.utils.data.Dataset, optional)** – An optional validation dataset\n",
        "\n",
        "**typeOf (str, optional)** – Determines the type of regulariser to be used. Default is ‘none’. For random regulariser use ‘Rand’. To use Facility Location set functiom as a regulariser use ‘FacLoc’. To use Diversity set functiom as a regulariser use ‘Diversity’.\n",
        "\n",
        "**lam (float, optional)** – Determines the amount of regularisation to be applied. Mandatory if is not typeOf=’none’ and by default set to None. For random regulariser use values should be between 0 and 1 as it determines fraction of points replaced by random points. For both ‘Diversity’ and ‘FacLoc’ lam determines the weightage given to them while computing the gain.\n",
        "\n",
        "**kernel_batch_size (int, optional)** – For 'Diversity' and 'FacLoc' regualrizer versions, similarity kernel is to be computed, which entails creating a 3d torch tensor of dimenssions $kernel\\_batch\\_size^{2}*(feature\\ dimenssion)$. Again kernel_batch_size should be such that one can exploit the benefits of tensorization while honouring the resourse constraits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Rff450Adx6"
      },
      "source": [
        "**Reinitialize Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cqjK5ok9k0CY"
      },
      "source": [
        "X_unlabeled = deepcopy(x_trn)\n",
        "y_unlabeled = deepcopy(y_trn)\n",
        "X_test = deepcopy(x_tst)\n",
        "y_test = deepcopy(y_tst)\n",
        "\n",
        "nSamps, dim = np.shape(X_unlabeled)\n",
        "\n",
        "np.random.seed(42)\n",
        "start_idxs = np.random.choice(nSamps, size=int(0.01*nSamps), replace=False)\n",
        "\n",
        "X_tr = X_unlabeled[start_idxs]\n",
        "X_unlabeled = np.delete(X_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "y_tr = y_unlabeled[start_idxs]\n",
        "y_unlabeled = np.delete(y_unlabeled, start_idxs, axis = 0)\n",
        "\n",
        "training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(x_tst), torch.tensor(y_tst, dtype=torch.long))\n",
        "\n",
        "n_rounds = 10    ##Number of rounds to run ac\n",
        "budget = int(0.01*x_trn.shape[0]) \n",
        "\n",
        "net = TwoLayerNet(data_dims, nclasses, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDPhhOTWpOzc"
      },
      "source": [
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iPoUboesk0CY"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 16, 'lr':float(0.01)}\n",
        "strategy = BADGE(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':500, 'lr':float(0.01),'batch_size':16, 'max_accuracy':0.99, 'window_size':20} \n",
        "dt = data_train(training_dataset, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(test_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    #Adding new points to training set\n",
        "    X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n",
        "    X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n",
        "\n",
        "    #Human In Loop, Assuming user adds new labels here\n",
        "    y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n",
        "    y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n",
        "    print('Number of training points -',X_tr.shape[0])\n",
        "\n",
        "    training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "    unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "\n",
        "    #Reload state and start training\n",
        "    strategy.update_data(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n",
        "    dt.update_data(training_dataset)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(test_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGcT3PKrAuQA"
      },
      "source": [
        "# **VISUALIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlea3j0v-Xvb"
      },
      "source": [
        "#Loading accuracies\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_gl = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "budget = 31 \n",
        "n_rounds = 10\n",
        "x_axis = np.array([31+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_gl, 'b-', label='GLISTER RAND',marker='o')\n",
        "plt.plot(x_axis, acc_en, 'g-', label='UNCERTAINITY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_rd, 'r', label='RANDOM',marker='o')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Images')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_IJCNN1')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}