{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distil_DEMO_CIFAR10_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WyIVzbwQgrwN",
        "W-Bxn6LmCeEI",
        "jm_ILTinrkLk",
        "BAGqAV0GrwwN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8AxA7KVfv9m"
      },
      "source": [
        "# **DISTIL Installation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bsVDupO_EOh"
      },
      "source": [
        "!git clone https://github.com/decile-team/distil.git\n",
        "!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ submodlib\n",
        "!cd distil && git checkout origin/scalable\n",
        "%cd distil/examples/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kchiw_zcf4tE"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7l1LUx6CSyx"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import cifar\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "sys.path.append('../')\n",
        "from distil.active_learning_strategies import GLISTER, BADGE, EntropySampling, RandomSampling\n",
        "\n",
        "from distil.utils.models.resnet import ResNet18\n",
        "from distil.utils.train_helper import data_train\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvn_Dal4gb8S"
      },
      "source": [
        "# **Data, Model & Directory Configuration**\n",
        "\n",
        "The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images and test set contains 10,000 images. We will use DISTIL's custom data handler for CIFAR10:- DataHandler_CIFAR10 to load the labeled as well as unlabeled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLy9wyjCuT-"
      },
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '../downloaded_data/'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWHNtN2rDtsZ"
      },
      "source": [
        "#Model Directory\n",
        "base_dir = \"/content/models/\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spOTFeWgfm4b"
      },
      "source": [
        "# **INITIAL MODEL TRAINING**\n",
        "Run only if you don't have a base model or when you are running it for the first time. Otherwise, you skip this cell and load model by running the below cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru5tj6u8fwEi"
      },
      "source": [
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, net, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyIVzbwQgrwN"
      },
      "source": [
        "# **LOAD BASE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd-ieb7RgyJY"
      },
      "source": [
        "base_dir = \"/content/models/\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Bxn6LmCeEI"
      },
      "source": [
        "# **RANDOM SAMPLING**\n",
        "This strategy is often used as a baseline, where we pick a set of unlabled points randomly. Here we create a instance of distil.active_learning_strategies.random_sampling.RandomSampling by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a wrapper around it that strips the label\n",
        "\n",
        "**net (class object)** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar.\n",
        "\n",
        "**nclasses (int)** – No. of classes in tha dataset\n",
        "\n",
        "**args (dictionary)**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8OdeQ63DZuP"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = RandomSampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    cifar10_full_train.transform = cifar_test_transform # Disable augmentation\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform # Enable augmentation\n",
        "\n",
        "    #Adding new points to training set\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_ILTinrkLk"
      },
      "source": [
        "# **Uncertanity based Active learning Strategy**\n",
        "\n",
        "The most basic active learning strategy, where we select samples about which the model is most uncertain. To quantify the uncertainity we use entropy, therefore select points which have maximum entropy. Let $z_i$ be output from the model then the correponding softmax would be $$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$. Then entropy can be calculated as, $$ENTROPY = -\\sum_j \\sigma(z_j)*log(\\sigma(z_j))$$\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.entropy_sampling.EntropySampling with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieS3rs-TC8bE"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5sn-ejd2IW4"
      },
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '../downloaded_data/'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bheCElpz2Q-5"
      },
      "source": [
        "base_dir = \"/content/models/\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3C0729a2Vzg"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = EntropySampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    cifar10_full_train.transform = cifar_test_transform # Disable augmentation\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform # Enable augmentation\n",
        "\n",
        "    #Adding new points to training set\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGqAV0GrwwN"
      },
      "source": [
        "# **BADGE**\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). Here at each around of selection loss gradients are computed using the hypothesised lables. Then to points to be labled are selected by applying k-means++ on these loss gradients. \n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.badge.BADGE with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z2N1gsCGE8U"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voTthJxV2kLB"
      },
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '../downloaded_data/'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF0BAQAL2kLB"
      },
      "source": [
        "base_dir = \"/content/models/\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0xEWpl2kLC"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = BADGE(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    cifar10_full_train.transform = cifar_test_transform # Disable augmentation\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform # Enable augmentation\n",
        "\n",
        "    #Adding new points to training set\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuIa_yJdiDOh"
      },
      "source": [
        "# **GLISTER**\n",
        "This is implemetation of GLISTER-ACTIVE from the paper [GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning](https://arxiv.org/abs/2012.10630). GLISTER methods tries to solve a bi-level optimisation problem.\n",
        "\\begin{equation*}\n",
        "\\overbrace{\\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmax\\hspace{0.7mm}}} LL_V(\\underbrace{\\underset{\\theta}{\\operatorname{argmax\\hspace{0.7mm}}} LL_T( \\theta, S)}_{inner-level}, {\\mathcal V})}^{outer-level}\n",
        "\\end{equation*}\n",
        "where is $S$ is set of points selected at each round,${\\mathcal V}$ could be a dedicated validation set with labled points or could be union of labeled and unlabeled points with hypothesised labels, $k$ is the budget.\n",
        "To set ${\\mathcal V}$ to be validation set, while calling **GLISTER** class in the toolkit set _valid=TRUE_ and pass validation set otherwise set _valid=False_.\n",
        "\n",
        "Solving this problem directly is almost impossible, therefore we resort to one-step approxiations.We start we $S^0$ as empty set and bulid it as $S^k = S^{k-1} \\cup e$, where $e$ is $\\underset{e}{\\operatorname{argmax\\hspace{0.7mm}}} G_{\\theta}(e | S^k)$. We define,$$G_{\\theta}(e | S^k) = LL_{V}(\\theta^{k}, {\\mathcal V})$$ and update $$\\theta^k \\leftarrow \\theta^{k-1} -  \\eta \\nabla_{\\theta} LL_T(\\hat{\\theta}, e)$$ where $\\hat{\\theta}$ is the parameters of the model at the begining of the selection.\n",
        "To prevent overfitting, we can add regularizer to GLISTER, which can be set by **_typeOf_**. **_typeOf_** can be set to - **'none'**(which is default) for normal GLISTER,**'Rand'** for replacing **_lam_** fraction of points replaced by random points, **'Diversity'** adding diversity set function while computing gain and **'FacLoc'** adding Facility Location set function while computing gain. **_lam_** for both **'Diversity'** and **'FacLoc'** determines the weightage given to them while computing the gain.\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.glister.GLISTER( with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling, we slight change that, **args** dictionary should have keys ‘batch_size’ and ‘lr’. ‘lr’ should be the learning rate used for training. In addition to those folowing additional parameters may be passed:\n",
        "\n",
        "**validation_dataset (torch.utils.data.Dataset, optional)** – An optional validation dataset\n",
        "\n",
        "**typeOf (str, optional)** – Determines the type of regulariser to be used. Default is ‘none’. For random regulariser use ‘Rand’. To use Facility Location set functiom as a regulariser use ‘FacLoc’. To use Diversity set functiom as a regulariser use ‘Diversity’.\n",
        "\n",
        "**lam (float, optional)** – Determines the amount of regularisation to be applied. Mandatory if is not typeOf=’none’ and by default set to None. For random regulariser use values should be between 0 and 1 as it determines fraction of points replaced by random points. For both ‘Diversity’ and ‘FacLoc’ lam determines the weightage given to them while computing the gain.\n",
        "\n",
        "**kernel_batch_size (int, optional)** – For 'Diversity' and 'FacLoc' regualrizer versions, similarity kernel is to be computed, which entails creating a 3d torch tensor of dimenssions $kernel\\_batch\\_size^{2}*(feature\\ dimenssion)$. Again kernel_batch_size should be such that one can exploit the benefits of tensorization while honouring the resourse constraits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hfo3xvHGJog"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaVPq19w2sWH"
      },
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '../downloaded_data/'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_target_transform = transforms.ToTensor()\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform, target_transform=torch.tensor)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform, target_transform=torch.tensor)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbkv74Cq2sWH"
      },
      "source": [
        "base_dir = \"/content/models/\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFg2EPMC2sWH"
      },
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 20, 'lr' : 0.01}\n",
        "strategy = GLISTER(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "#Initial Training\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "#Updating the trained model in strategy class\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# y_pred = strategy.predict(X_test).numpy()\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "##User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    #Using select function for getting next set of data points\n",
        "    cifar10_full_train.transform = cifar_test_transform # Disable augmentation\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform # Enable augmentation\n",
        "\n",
        "    #Adding new points to training set\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZjPPO7java"
      },
      "source": [
        "**VISUALISATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vn65GnhZosp"
      },
      "source": [
        "#Loading accuracies\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_gl = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "x_axis = np.array([train_size+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_gl, 'b-', label='GLISTER RAND',marker='o')\n",
        "plt.plot(x_axis, acc_en, 'g-', label='UNCERTAINITY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_rd, 'r', label='RANDOM',marker='o')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Images')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_CIFAR10')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}